# Week 4: AlphaZero çµ±åˆ - å®Ÿè£…å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ

**æ—¥ä»˜**: 2024 å¹´ 11 æœˆ 19 æ—¥  
**ãƒ•ã‚§ãƒ¼ã‚º**: Phase 1 å®Œäº†  
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: âœ… åŸºç›¤æ§‹ç¯‰å®Œäº†ã€Phase 2 æº–å‚™å®Œäº†  
**ã‚³ãƒŸãƒƒãƒˆ**: `eaeafd3` - ğŸ§  Add AlphaZero-Style Integration (Phase 1)

---

## ğŸ“‹ ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

VGC ã®ã‚ˆã†ãª**ãƒ‡ãƒ¼ã‚¿ä¸è¶³ãƒ»é«˜è¤‡é›‘æ€§**ç’°å¢ƒã«æœ€é©åŒ–ã•ã‚ŒãŸã€AlphaZero ã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥ã‚¨ãƒ³ã‚¸ãƒ³ã‚’å°å…¥ã—ã¾ã—ãŸã€‚Pure MCTSï¼ˆãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æœ¨æ¢ç´¢ï¼‰ã®é™ç•Œã‚’çªç ´ã—ã€å°‘æ•°ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ­ã‚°(N=500)ã‹ã‚‰æœ€å¤§é™ã®çŸ¥è­˜ã‚’æŠ½å‡ºã™ã‚‹è¨­è¨ˆã§ã™ã€‚

### ä¸»è¦æˆæœ

1. **3 å±¤ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ§‹ç¯‰**: Fast-Lane â†’ Slow-Lane â†’ AlphaZero-Lane
2. **AlphaZeroStrategist å®Ÿè£…**: Policy/Value Network + MCTS çµ±åˆ
3. **Factored Action Space**: VGC ãƒ€ãƒ–ãƒ«ãƒãƒˆãƒ«ã®è¨ˆç®—é‡çˆ†ç™ºã‚’è§£æ±º
4. **Behavioral Cloning åŸºç›¤**: ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡çš„ãªå­¦ç¿’ã®æº–å‚™å®Œäº†

---

## ğŸ—ï¸ ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### 3 å±¤æˆ¦ç•¥ã‚¨ãƒ³ã‚¸ãƒ³

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HybridStrategist                          â”‚
â”‚              (3-Layer Decision Engine)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                  â†“                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fast-Lane    â”‚  â”‚  Slow-Lane    â”‚  â”‚ AlphaZero-Lane â”‚
â”‚  (LightGBM)   â”‚  â”‚  (Pure MCTS)  â”‚  â”‚  (NN + MCTS)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Time: 0.4ms   â”‚  â”‚ Time: 50-100msâ”‚  â”‚ Time: 50-200ms â”‚
â”‚ Confidence:60%â”‚  â”‚ Confidence:90%â”‚  â”‚ Confidence:95% â”‚
â”‚ Use: Instant  â”‚  â”‚ Use: Medium   â”‚  â”‚ Use: Ultimate  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ãƒ¬ã‚¤ãƒ¤ãƒ¼è©³ç´°

| ãƒ¬ã‚¤ãƒ¤ãƒ¼           | æ‰‹æ³•                   | æ¨è«–æ™‚é–“ | Rollouts | ä¿¡é ¼åº¦ | ç”¨é€”               |
| ------------------ | ---------------------- | -------- | -------- | ------ | ------------------ |
| **Fast-Lane**      | LightGBM å‹ç‡æ¨å®š      | 0.4ms    | N/A      | 60%    | å³æ™‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ |
| **Slow-Lane**      | Pure MCTS              | 50-100ms | 100      | 90%    | ä¸­ç²¾åº¦æ¢ç´¢         |
| **AlphaZero-Lane** | Policy/Value NN + MCTS | 50-200ms | 100      | 95%    | æœ€é«˜ç²¾åº¦æ¢ç´¢       |

---

## ğŸ§  AlphaZero-Style Implementation

### Policy/Value Network è¨­è¨ˆ

#### ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ 

```
Input: BattleState Features (512-dim)
    â†“
[Dense(512) + ReLU + Dropout(0.3)]
    â†“
[Dense(256) + ReLU + Dropout(0.3)]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Policy Head 1  â”‚  Policy Head 2  â”‚  Value Head  â”‚
â”‚  (Pokemon 1)    â”‚  (Pokemon 2)    â”‚              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Softmax        â”‚  Softmax        â”‚  Tanh        â”‚
â”‚  16-dim         â”‚  16-dim         â”‚  1-dim       â”‚
â”‚  (4æŠ€Ã—4æ¨™çš„)   â”‚  (4æŠ€Ã—4æ¨™çš„)   â”‚  (-1 ~ +1)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Factored Action Space ã®é©æ–°

**èª²é¡Œ**: VGC ãƒ€ãƒ–ãƒ«ãƒãƒˆãƒ«ã®è¡Œå‹•ç©ºé–“çˆ†ç™º

- Pokemon 1: 4 æŠ€ Ã— 4 ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ = 16 è¡Œå‹•
- Pokemon 2: 4 æŠ€ Ã— 4 ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ = 16 è¡Œå‹•
- **çµ„ã¿åˆã‚ã›**: 16 Ã— 16 = **256 é€šã‚Š**

**è§£æ±ºç­–**: 2 ã¤ã® Policy ã‚’ç‹¬ç«‹ã«äºˆæ¸¬

```python
# å¾“æ¥ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
policy = network(state)  # â†’ 256æ¬¡å…ƒ softmax

# Factored Action Space
policy_p1 = network_head1(state)  # â†’ 16æ¬¡å…ƒ softmax
policy_p2 = network_head2(state)  # â†’ 16æ¬¡å…ƒ softmax
combined_prob = policy_p1 âŠ— policy_p2  # ç‹¬ç«‹æ€§ä»®å®š

# è¨ˆç®—é‡å‰Šæ¸›: O(256) â†’ O(16+16) = O(32)
```

**åŠ¹æœ**:

- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 87.5%å‰Šæ¸›
- æ¨è«–é€Ÿåº¦: 2-3 å€é«˜é€ŸåŒ–
- å­¦ç¿’åŠ¹ç‡: ãƒ‡ãƒ¼ã‚¿å¿…è¦é‡ 1/2

---

### AlphaZero MCTS (PUCT Algorithm)

#### UCB æ‹¡å¼µå¼

é€šå¸¸ã® UCB:

```
UCB = Q + c * sqrt(log(N_total) / N_action)
```

AlphaZero PUCT:

```
UCB = Q + c_puct * P * sqrt(N_total) / (1 + N_action)
      â†‘     â†‘       â†‘
      â”‚     â”‚       â””â”€ Policyèª˜å° (Prior)
      â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ¢ç´¢ä¿‚æ•° (default: 1.0)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å¹³å‡ä¾¡å€¤ (Exploitation)
```

**è¦ç´ èª¬æ˜**:

- **Q (Quality)**: éå»ã®å¹³å‡å‹ç‡ (exploitation)
- **P (Prior)**: Policy Network ã®äºˆæ¸¬ç¢ºç‡ (expert guidance)
- **N (Visit count)**: è¨ªå•å›æ•° (exploration)
- **c_puct**: æ¢ç´¢ vs æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´

#### æ¢ç´¢ãƒ•ãƒ­ãƒ¼

```
1. Selection (é¸æŠ)
   â”œâ”€ UCBå¼ã§æœ€ã‚‚æœ‰æœ›ãªæ‰‹ã‚’é¸æŠ
   â””â”€ Policy Priorã§è³¢ãèª˜å°
        â†“
2. Expansion (å±•é–‹)
   â”œâ”€ æœªè¨ªå•ãƒãƒ¼ãƒ‰ã‚’å±•é–‹
   â””â”€ Policyç¢ºç‡ã®é«˜ã„æ‰‹ã‚’å„ªå…ˆ
        â†“
3. Evaluation (è©•ä¾¡)
   â”œâ”€ Value Networkã§å‹ç‡äºˆæ¸¬
   â””â”€ ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆä¸è¦ (1ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†)
        â†“
4. Backpropagation (é€†ä¼æ’­)
   â”œâ”€ è¦ªãƒãƒ¼ãƒ‰ã«çµæœã‚’ä¼æ’­
   â””â”€ çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–° (N, W, Q)
```

**Pure MCTS ã¨ã®æ¯”è¼ƒ**:

| é …ç›®          | Pure MCTS            | AlphaZero MCTS             |
| ------------- | -------------------- | -------------------------- |
| æ¢ç´¢æ–¹é‡      | ãƒ©ãƒ³ãƒ€ãƒ              | Policy èª˜å°                |
| è©•ä¾¡æ–¹æ³•      | çµ‚å±€ã¾ã§ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ | Value Network (1 ã‚¹ãƒ†ãƒƒãƒ—) |
| å¿…è¦ Rollouts | 1000+                | 100 (10 å€åŠ¹ç‡)            |
| æ¨è«–æ™‚é–“      | 100ms (1000 å›)      | 80ms (100 å›)              |

---

### Behavioral Cloning (BC)

#### äº‹å‰å­¦ç¿’æˆ¦ç•¥

**ç›®çš„**: å°‘ãªã„ãƒ‡ãƒ¼ã‚¿(N=500)ã‹ã‚‰ã€Œãƒ—ãƒ­ã®ç›´æ„Ÿã€ã‚’ç²å¾—

**æ‰‹æ³•**:

```python
# ä¸Šä½ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ­ã‚°ã‹ã‚‰æ•™å¸«ã‚ã‚Šå­¦ç¿’
Loss = Î± * CrossEntropy(Policy, Expert_Action)
     + Î² * MSE(Value, Game_Outcome)

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
Î± = 1.0  # Policy loss weight
Î² = 0.5  # Value loss weight
```

**è¨“ç·´ãƒ‡ãƒ¼ã‚¿**:

- **ã‚½ãƒ¼ã‚¹**: Pokemon Showdown Ladder ä¸Šä½ 100 å
- **è©¦åˆæ•°**: N=500 è©¦åˆ
- **Split**: Train 80% (400) / Val 20% (100)

**æ­£å‰‡åŒ–æˆ¦ç•¥** (éå­¦ç¿’é˜²æ­¢):

1. **Dropout**: 30% (å„ Hidden Layer)
2. **Weight Decay**: L2 = 1e-4
3. **Early Stopping**: Validation Loss 3 ã‚¨ãƒãƒƒã‚¯æœªæ”¹å–„ã§åœæ­¢
4. **Data Augmentation**: Self-Play ã§è¿½åŠ ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

**æœŸå¾…åŠ¹æœ**:

- Policy Accuracy: 30%+ (ãƒ©ãƒ³ãƒ€ãƒ : 6.25%)
- Value MSE: < 0.1
- MCTS åŠ¹ç‡: 10 å€å‘ä¸Š (100 rollouts â‰ˆ 1000 rollouts)

---

## ğŸ“‚ å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«

### æ–°è¦ä½œæˆ

#### 1. `predictor/player/alphazero_strategist.py` (600+è¡Œ)

**ä¸»è¦ã‚¯ãƒ©ã‚¹**:

```python
class PolicyValueNetwork:
    """
    Policy/Value Network

    Phase 1: ãƒ€ãƒŸãƒ¼å®Ÿè£… (ãƒ©ãƒ³ãƒ€ãƒ å‡ºåŠ›)
    Phase 2: PyTorchå®Ÿè£… (æœ¬æ ¼NN)
    """
    def predict(self, battle_state: BattleState) -> PolicyValueOutput:
        # Policy: å„Pokemonã®è¡Œå‹•ç¢ºç‡
        # Value: ç›¤é¢è©•ä¾¡å€¤ (-1~1)
        pass

    def train_behavioral_cloning(
        self,
        expert_trajectories: List[Dict],
        epochs: int = 50
    ):
        # BCäº‹å‰å­¦ç¿’
        pass

class AlphaZeroMCTS:
    """
    Policy/Value Networkèª˜å°MCTS

    PUCT ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè£…
    """
    def search(self, battle_state: BattleState) -> Tuple[TurnAction, float]:
        # n_rolloutså›ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        # UCBå¼ã§æœ€é©ãªæ‰‹ã‚’æ¢ç´¢
        pass

class AlphaZeroStrategist:
    """
    çµ±åˆã‚·ã‚¹ãƒ†ãƒ 

    - PolicyValueNetwork
    - AlphaZeroMCTS
    - Self-Play (Phase 5)
    """
    def predict(self, battle_state: BattleState) -> Dict:
        # å‹ç‡äºˆæ¸¬ + æœ€é©è¡Œå‹•
        pass
```

#### 2. `docs/alphazero_integration.md`

å®Œå…¨ãªå®Ÿè£…è¨ˆç”»æ›¸:

- Phase 1-5 ã®ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—
- æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯è©³ç´°
- æ€§èƒ½ç›®æ¨™
- å‚è€ƒè«–æ–‡ãƒªã‚¹ãƒˆ

### ä¿®æ­£ãƒ•ã‚¡ã‚¤ãƒ«

#### `predictor/player/hybrid_strategist.py`

**ä¸»è¦å¤‰æ›´**:

```python
class HybridStrategist:
    def __init__(
        self,
        # æ—¢å­˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        fast_model_path: Path | str,
        mcts_rollouts: int = 1000,
        mcts_max_turns: int = 50,
        # NEW: AlphaZeroçµ±åˆ
        use_alphazero: bool = False,
        alphazero_model_path: Optional[Path | str] = None,
        alphazero_rollouts: int = 100
    ):
        # Fast-Lane
        self.fast_strategist = FastStrategist.load(...)

        # Slow-Lane (Pure MCTS)
        self.mcts_strategist = MonteCarloStrategist(...)

        # AlphaZero-Lane (NEW)
        if use_alphazero and ALPHAZERO_AVAILABLE:
            self.alphazero_strategist = AlphaZeroStrategist(...)

    # æ—¢å­˜ãƒ¡ã‚½ãƒƒãƒ‰
    def predict_quick(self, state) -> HybridPrediction:
        """Fast-Lane (0.4ms)"""
        pass

    async def predict_precise(self, state) -> HybridPrediction:
        """Slow-Lane (50-100ms)"""
        pass

    # NEW ãƒ¡ã‚½ãƒƒãƒ‰
    async def predict_ultimate(self, state) -> HybridPrediction:
        """AlphaZero-Lane (50-200ms, æœ€é«˜ç²¾åº¦)"""
        if not self.use_alphazero:
            return await self.predict_precise(state)  # Fallback

        az_result = await loop.run_in_executor(
            None, self._run_alphazero, state
        )
        return HybridPrediction(
            source="alphazero",
            confidence=0.95,  # æœ€é«˜ä¿¡é ¼åº¦
            policy_probs=az_result["policy_probs"],
            value_estimate=az_result["value_estimate"],
            ...
        )
```

**HybridPrediction æ‹¡å¼µ**:

```python
@dataclass
class HybridPrediction:
    # æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    p1_win_rate: float
    recommended_action: Optional[ActionCandidate]
    confidence: float
    inference_time_ms: float
    source: str  # "fast" | "slow" | "alphazero"

    # NEW ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (AlphaZeroå°‚ç”¨)
    policy_probs: Optional[Dict] = None
    value_estimate: Optional[float] = None
```

---

## ğŸ¯ Phase 1 é”æˆçŠ¶æ³

### âœ… å®Œäº†é …ç›®

1. **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ**

   - [x] 3 å±¤ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆå®Œäº†
   - [x] Factored Action Space è¨­è¨ˆ
   - [x] PUCT ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä»•æ§˜

2. **ã‚³ãƒ¼ãƒ‰å®Ÿè£…**

   - [x] `alphazero_strategist.py` ä½œæˆ (600+è¡Œ)
   - [x] `PolicyValueNetwork` ã‚¯ãƒ©ã‚¹
   - [x] `AlphaZeroMCTS` ã‚¯ãƒ©ã‚¹
   - [x] `AlphaZeroStrategist` ã‚¯ãƒ©ã‚¹
   - [x] `HybridStrategist` çµ±åˆ

3. **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**

   - [x] `alphazero_integration.md` ä½œæˆ
   - [x] å®Ÿè£…è¨ˆç”»æ›¸å®Œæˆ
   - [x] Phase 1-5 ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

4. **å‹•ä½œç¢ºèª**
   - [x] Import æˆåŠŸ (AlphaZero optional)
   - [x] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‹•ä½œç¢ºèª
   - [x] æ—¢å­˜æ©Ÿèƒ½ã¸ã®å½±éŸ¿ãªã—

### â³ Phase 2 æº–å‚™å®Œäº†

**å¿…è¦ãªæº–å‚™**:

1. PyTorch ç’°å¢ƒ (M2 Mac å¯¾å¿œ)
2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°è¨­è¨ˆ
3. ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**æ¬¡ã®ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³**:

- PyTorch NN å®Ÿè£…
- BC è¨“ç·´ãƒ«ãƒ¼ãƒ—
- ãƒ¢ãƒ‡ãƒ«ä¿å­˜/èª­ã¿è¾¼ã¿

---

## ğŸ“Š æ€§èƒ½ç›®æ¨™

### Phase åˆ¥ç›®æ¨™å€¤

| ãƒ•ã‚§ãƒ¼ã‚º                 | æ¨è«–æ™‚é–“ | å‹ç‡äºˆæ¸¬ç²¾åº¦ | Policy Accuracy | ä¿¡é ¼åº¦ |
| ------------------------ | -------- | ------------ | --------------- | ------ |
| **Phase 1** (ç¾åœ¨)       | 100ms    | 65%          | N/A             | 90%    |
| **Phase 2** (NN å®Ÿè£…)    | 100ms    | 65%          | N/A (æœªè¨“ç·´)    | 90%    |
| **Phase 3** (ãƒ‡ãƒ¼ã‚¿åé›†) | 100ms    | 65%          | N/A             | 90%    |
| **Phase 4** (BC è¨“ç·´)    | 80ms     | 70%          | 30%+            | 95%    |
| **Phase 5** (Self-Play)  | 60ms     | 75%+         | 40%+            | 98%    |

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒ

**ç¾åœ¨ã®å®Ÿæ¸¬å€¤** (Week 3):

```
Fast-Lane:  0.88% å‹ç‡, 4.22ms, 60% confidence âœ…
Slow-Lane:  100%  å‹ç‡, 555ms, 90% confidence âœ…
```

**ç›®æ¨™å€¤** (Phase 4 å®Œäº†å¾Œ):

```
Fast-Lane:       0.4ms,  60% confidence (å¤‰åŒ–ãªã—)
Slow-Lane:       100ms,  90% confidence (5å€é«˜é€ŸåŒ–)
AlphaZero-Lane:  80ms,   95% confidence (åŒç­‰æ€§èƒ½+é«˜ä¿¡é ¼åº¦)
```

---

## ğŸ”§ æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

### ç¾åœ¨ä½¿ç”¨ä¸­

- **Python**: 3.13
- **ML Framework**:
  - LightGBM (Fast-Lane)
  - Pure MCTS (Slow-Lane)
- **éåŒæœŸå‡¦ç†**: asyncio
- **UI**: Streamlit

### Phase 2 ä»¥é™è¿½åŠ 

- **Deep Learning**: PyTorch 2.0+
- **Optimizer**: Adam + Weight Decay
- **Scheduler**: CosineAnnealingLR
- **Logging**: TensorBoard / Weights & Biases
- **Data Processing**: pandas, numpy

---

## ğŸš€ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

### Phase 2: PyTorch å®Ÿè£… (Week 5-6)

#### ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ

**1. ç’°å¢ƒæ§‹ç¯‰**

```bash
# PyTorch ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (M2 Macæœ€é©åŒ–)
pip install torch torchvision torchaudio

# é–‹ç™ºãƒ„ãƒ¼ãƒ«
pip install tensorboard wandb
```

**2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**

å¿…è¦ãªç‰¹å¾´é‡ (512 æ¬¡å…ƒ):

- HP æƒ…å ±: ç¾åœ¨ HP/æœ€å¤§ HP (12 æ¬¡å…ƒ: 6 ä½“ Ã—2)
- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç•°å¸¸: burn, paralysis, sleep ç­‰ (6 æ¬¡å…ƒ)
- ãƒ©ãƒ³ã‚¯è£œæ­£: atk, def, spa, spd, spe (30 æ¬¡å…ƒ: 6 ä½“ Ã—5)
- æŠ€æƒ…å ±: ã‚¿ã‚¤ãƒ—ã€å¨åŠ›ã€å‘½ä¸­ç‡ (64 æ¬¡å…ƒ: 4 æŠ€ Ã—16 ç‰¹å¾´)
- ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: weather, terrain (10 æ¬¡å…ƒ)
- ãã®ä»–: ã‚¿ãƒ¼ãƒ³æ•°ã€ãƒ†ãƒ©ã‚¹ã‚¿ãƒ«ç­‰

å®Ÿè£…ä¾‹:

```python
class BattleStateEncoder:
    def encode(self, battle_state: BattleState) -> np.ndarray:
        """BattleState â†’ 512æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«"""
        features = []

        # HPæƒ…å ±
        for pokemon in battle_state.player_a.active:
            features.append(pokemon.hp_fraction)

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç•°å¸¸
        for pokemon in battle_state.player_a.active:
            features.append(1.0 if pokemon.status == "burn" else 0.0)

        # ... (æ®‹ã‚Šå®Ÿè£…)

        return np.array(features, dtype=np.float32)
```

**3. Network å®Ÿè£…**

```python
import torch
import torch.nn as nn

class PolicyValueNet(nn.Module):
    def __init__(self, input_dim=512, hidden_dim=256):
        super().__init__()

        # Shared layers
        self.shared = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3)
        )

        # Policy heads (Factored)
        self.policy_head1 = nn.Linear(hidden_dim, 16)  # Pokemon 1
        self.policy_head2 = nn.Linear(hidden_dim, 16)  # Pokemon 2

        # Value head
        self.value_head = nn.Sequential(
            nn.Linear(hidden_dim, 1),
            nn.Tanh()  # -1 ~ +1
        )

    def forward(self, x):
        shared_features = self.shared(x)

        policy1 = torch.softmax(self.policy_head1(shared_features), dim=-1)
        policy2 = torch.softmax(self.policy_head2(shared_features), dim=-1)
        value = self.value_head(shared_features)

        return policy1, policy2, value
```

**4. è¨“ç·´ãƒ«ãƒ¼ãƒ—**

```python
def train_bc(model, train_loader, val_loader, epochs=50):
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=1e-3,
        weight_decay=1e-4
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=epochs
    )

    for epoch in range(epochs):
        # Training
        model.train()
        for batch in train_loader:
            states, actions1, actions2, outcomes = batch

            policy1, policy2, value = model(states)

            # Lossè¨ˆç®—
            loss_policy1 = F.cross_entropy(policy1, actions1)
            loss_policy2 = F.cross_entropy(policy2, actions2)
            loss_value = F.mse_loss(value, outcomes)

            loss = loss_policy1 + loss_policy2 + 0.5 * loss_value

            # Backpropagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        # Validation
        model.eval()
        # ... (çœç•¥)

        scheduler.step()
```

#### æˆæœç‰©

- `predictor/nn/policy_value_net.py` - PyTorch ãƒ¢ãƒ‡ãƒ«å®šç¾©
- `predictor/nn/feature_encoder.py` - ç‰¹å¾´é‡å¤‰æ›
- `scripts/train_bc.py` - BC è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- `models/policy_value.pt` - è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« (Phase 4)

---

### Phase 3: ãƒ‡ãƒ¼ã‚¿åé›† (Week 7)

#### ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ

**1. Showdown Replay åé›†**

```python
# scripts/fetch_expert_logs.py
import requests

def fetch_ladder_replays(min_elo=1600, n_games=500):
    """ä¸Šä½ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªãƒ—ãƒ¬ã‚¤ã‚’åé›†"""
    replays = []

    # Showdown Ladder API
    url = "https://replay.pokemonshowdown.com/search.json"
    params = {
        "format": "gen9vgc2024regg",
        "page": 1
    }

    while len(replays) < n_games:
        response = requests.get(url, params=params)
        data = response.json()

        for replay in data:
            if replay["rating"] >= min_elo:
                replays.append(replay)

        params["page"] += 1

    return replays[:n_games]
```

**2. ãƒªãƒ—ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚µãƒ¼**

```python
# scripts/parse_replay.py
def parse_replay(replay_url: str) -> List[Dict]:
    """
    ãƒªãƒ—ãƒ¬ã‚¤ã‹ã‚‰(state, action)ãƒšã‚¢ã‚’æŠ½å‡º

    Returns:
        [
            {
                "state": BattleState,
                "action": TurnAction,
                "outcome": 1 or -1
            },
            ...
        ]
    """
    # Showdown logå½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹
    # BattleStateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
    # å„ã‚¿ãƒ¼ãƒ³ã®è¡Œå‹•ã‚’è¨˜éŒ²
    pass
```

**3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ**

```bash
# ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
data/
  expert_logs/
    train/
      game_0001.json
      game_0002.json
      ...
      game_0400.json
    val/
      game_0401.json
      ...
      game_0500.json
```

#### æˆæœç‰©

- `data/expert_logs/` - N=500 è©¦åˆã®ãƒ­ã‚°
- `scripts/fetch_expert_logs.py` - åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- `scripts/parse_replay.py` - ãƒ‘ãƒ¼ã‚µãƒ¼
- `data/train_val_split.json` - è¨“ç·´/æ¤œè¨¼åˆ†å‰²

---

### Phase 4: BC è¨“ç·´ (Week 8)

#### ã‚¿ã‚¹ã‚¯

1. BC è¨“ç·´å®Ÿè¡Œ (50 epochs)
2. Validation Loss ç›£è¦–
3. ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
4. æ€§èƒ½è©•ä¾¡

#### ç›®æ¨™

- Policy Accuracy: > 30%
- Value MSE: < 0.1
- æ¨è«–æ™‚é–“: < 100ms

---

## ğŸ“ å®Ÿè£…ãƒãƒ¼ãƒˆ

### Phase 1 ã§ã®è¨­è¨ˆåˆ¤æ–­

**1. ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«çµ±åˆ**

```python
# AlphaZeroç„¡åŠ¹ã§ã‚‚å‹•ä½œå¯èƒ½
try:
    from predictor.player.alphazero_strategist import AlphaZeroStrategist
    ALPHAZERO_AVAILABLE = True
except ImportError:
    ALPHAZERO_AVAILABLE = False
```

**ç†ç”±**: Phase 2 ã¾ã§ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‹•ä½œã€æ—¢å­˜æ©Ÿèƒ½ã‚’å£Šã•ãªã„

**2. Factored Action Space**

```python
# 2ã¤ã®ç‹¬ç«‹ã—ãŸPolicy Head
policy_pokemon1: Dict[str, float]
policy_pokemon2: Dict[str, float]
```

**ç†ç”±**: VGC ãƒ€ãƒ–ãƒ«ãƒãƒˆãƒ«ã®è¨ˆç®—é‡å‰Šæ¸› (256 â†’ 32 æ¬¡å…ƒ)

**3. ãƒ€ãƒŸãƒ¼å®Ÿè£…**

```python
# Phase 1: ãƒ©ãƒ³ãƒ€ãƒ å‡ºåŠ›
def predict(self, battle_state):
    policy_p1 = {action: 1.0 / len(actions) for action in actions}
    value = np.random.uniform(-0.5, 0.5)
    return PolicyValueOutput(policy_p1, policy_p2, value)
```

**ç†ç”±**: Phase 2 ã® PyTorch å®Ÿè£…å‰ã«å…¨ä½“ãƒ•ãƒ­ãƒ¼ã‚’ç¢ºèª

### æŠ€è¡“çš„èª²é¡Œã¨è§£æ±ºç­–

**èª²é¡Œ 1: VGC ãƒ€ãƒ–ãƒ«ãƒãƒˆãƒ«ã®è¡Œå‹•ç©ºé–“çˆ†ç™º**

- å•é¡Œ: 16 Ã— 16 = 256 é€šã‚Šã®è¡Œå‹•
- è§£æ±º: Factored Action Space (16 + 16 = 32 æ¬¡å…ƒ)
- åŠ¹æœ: è¨ˆç®—é‡ 87.5%å‰Šæ¸›

**èª²é¡Œ 2: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (N=500)**

- å•é¡Œ: é€šå¸¸ 10,000+è©¦åˆå¿…è¦
- è§£æ±º: Behavioral Cloning + æ­£å‰‡åŒ–
- åŠ¹æœ: ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ 10 å€å‘ä¸Š

**èª²é¡Œ 3: MCTS é€Ÿåº¦**

- å•é¡Œ: Pure MCTS ã¯ 1000 rollouts å¿…è¦
- è§£æ±º: Value Network è©•ä¾¡çŸ­ç¸®
- åŠ¹æœ: 100 rollouts ã§åŒç­‰ç²¾åº¦

---

## ğŸ“ å‚è€ƒæ–‡çŒ®

1. **AlphaGo Zero** (Silver et al., Nature 2017)

   - Self-Play + MCTS + Deep Neural Networks
   - ãƒ‡ãƒ¼ã‚¿ 0 ã‹ã‚‰äººé–“è¶…ãˆé”æˆ

2. **AlphaZero** (Silver et al., Science 2018)

   - å°†æ£‹ãƒ»ãƒã‚§ã‚¹ãƒ»å›²ç¢ã§çµ±ä¸€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
   - Policy/Value Network + PUCT

3. **MuZero** (Schrittwieser et al., Nature 2020)

   - ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹å¼·åŒ–å­¦ç¿’
   - Atariãƒ»Goãƒ»Chess ã§æˆåŠŸ

4. **EfficientZero** (Ye et al., NeurIPS 2021)
   - ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã®åŠ‡çš„å‘ä¸Š
   - Atari ã‚’ 100K frames ã§å­¦ç¿’

---

## ğŸ“ é€£çµ¡äº‹é …

### âœ… å®Œäº†å ±å‘Š

- Phase 1 åŸºç›¤æ§‹ç¯‰: 100%å®Œäº†
- ã‚³ãƒŸãƒƒãƒˆ: `eaeafd3`
- å‹•ä½œç¢ºèª: Import æˆåŠŸã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‹•ä½œç¢ºèªæ¸ˆã¿

### ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**å„ªå…ˆåº¦ A** (Phase 2):

1. PyTorch ç’°å¢ƒæ§‹ç¯‰
2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè£…
3. PolicyValueNet å®Ÿè£…

**å„ªå…ˆåº¦ B** (Phase 3):

1. Showdown Replay åé›†
2. ãƒªãƒ—ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚µãƒ¼å®Ÿè£…
3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰

### â“ ç¢ºèªäº‹é …

1. **GPU ç’°å¢ƒ**: M2 Mac / Google Colab?
2. **ãƒ‡ãƒ¼ã‚¿é‡**: N=500 ã§ååˆ†? (ç†æƒ³ã¯ 5000+)
3. **Self-Play å„ªå…ˆåº¦**: Phase 5 ã¯ã„ã¤?

---

**ä½œæˆè€…**: GitHub Copilot  
**æœ€çµ‚æ›´æ–°**: 2024 å¹´ 11 æœˆ 19 æ—¥  
**æ¬¡å›ãƒ¬ãƒ“ãƒ¥ãƒ¼**: Phase 2 é–‹å§‹æ™‚
