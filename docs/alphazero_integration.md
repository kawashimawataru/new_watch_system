# AlphaZero-Style Integration Plan

**ä½œæˆæ—¥**: 2024 å¹´ 11 æœˆ 19 æ—¥  
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: Phase 1 (åŸºç›¤æ§‹ç¯‰ä¸­)  
**å„ªå…ˆåº¦**: HIGH ğŸ”¥

---

## ğŸ“‹ æ¦‚è¦

VGC ã®ã‚ˆã†ãª**ãƒ‡ãƒ¼ã‚¿ä¸è¶³ãƒ»é«˜è¤‡é›‘æ€§**ã®ç’°å¢ƒã«æœ€é©åŒ–ã•ã‚ŒãŸã€AlphaZero ã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥ã‚’å°å…¥ã—ã¾ã™ã€‚

### ç›®çš„

1. **Pure MCTS ã®é™ç•Œçªç ´**: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã ã‘ã§ã¯ä¸ååˆ†
2. **ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã®æœ€å¤§åŒ–**: N=500 è©¦åˆã®ãƒ­ã‚°ã‹ã‚‰æœ€å¤§é™ã®çŸ¥è­˜ã‚’æŠ½å‡º
3. **æ¢ç´¢åŠ¹ç‡ã®é£›èºçš„å‘ä¸Š**: Policy èª˜å°ã§ç„¡é§„ãªæ¢ç´¢ã‚’å‰Šæ¸›

---

## ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### 3 å±¤æˆ¦ç•¥ã‚·ã‚¹ãƒ†ãƒ 

| ãƒ¬ã‚¤ãƒ¤ãƒ¼           | æ‰‹æ³•                   | æ¨è«–æ™‚é–“ | ä¿¡é ¼åº¦ | ç”¨é€”               |
| ------------------ | ---------------------- | -------- | ------ | ------------------ |
| **Fast-Lane**      | LightGBM               | 0.4ms    | 60%    | å³æ™‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ |
| **Slow-Lane**      | Pure MCTS              | 50-100ms | 90%    | ä¸­ç²¾åº¦æ¢ç´¢         |
| **AlphaZero-Lane** | Policy/Value NN + MCTS | 50-200ms | 95%    | æœ€é«˜ç²¾åº¦æ¢ç´¢       |

### ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

```
Battle State
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Fast-Lane (LightGBM)                â”‚
â”‚   â€¢ 0.4ms å³æ™‚å¿œç­”                    â”‚
â”‚   â€¢ ä¿¡é ¼åº¦: 60%                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (éåŒæœŸ)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Slow-Lane (Pure MCTS)               â”‚
â”‚   â€¢ 100 rollouts                      â”‚
â”‚   â€¢ 50-100ms                          â”‚
â”‚   â€¢ ä¿¡é ¼åº¦: 90%                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (éåŒæœŸ)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AlphaZero-Lane (NN + MCTS)         â”‚
â”‚   â€¢ Policy Network èª˜å°æ¢ç´¢          â”‚
â”‚   â€¢ Value Network è©•ä¾¡çŸ­ç¸®           â”‚
â”‚   â€¢ 50-200ms (100 rollouts)          â”‚
â”‚   â€¢ ä¿¡é ¼åº¦: 95%                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Policy/Value Network

### ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ 

```python
Input: BattleStateç‰¹å¾´é‡ (512æ¬¡å…ƒ)
    â†“
[Dense(512) + ReLU + Dropout(0.3)]
    â†“
[Dense(256) + ReLU + Dropout(0.3)]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Policy Head 1  â”‚  Policy Head 2  â”‚  Value Head  â”‚
â”‚  (Pokemon 1)    â”‚  (Pokemon 2)    â”‚              â”‚
â”‚  Softmax        â”‚  Softmax        â”‚  Tanh        â”‚
â”‚  4æŠ€ Ã— 4æ¨™çš„   â”‚  4æŠ€ Ã— 4æ¨™çš„   â”‚  -1 ~ +1     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Factored Action Space

**VGC ã®èª²é¡Œ**: ãƒ€ãƒ–ãƒ«ãƒãƒˆãƒ«ã§ã¯è¡Œå‹•ç©ºé–“ãŒè†¨å¤§

- Pokemon 1: 4 æŠ€ Ã— 4 ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ = 16 è¡Œå‹•
- Pokemon 2: 4 æŠ€ Ã— 4 ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ = 16 è¡Œå‹•
- **çµ„ã¿åˆã‚ã›**: 16 Ã— 16 = **256 é€šã‚Š**

**è§£æ±ºç­–**: 2 ã¤ã® Policy ã‚’ç‹¬ç«‹ã«äºˆæ¸¬

- Policy Head 1: Pokemon 1 ã®è¡Œå‹•ç¢ºç‡ (16 æ¬¡å…ƒ)
- Policy Head 2: Pokemon 2 ã®è¡Œå‹•ç¢ºç‡ (16 æ¬¡å…ƒ)
- **è¨ˆç®—é‡å‰Šæ¸›**: 256 â†’ 16 + 16 = **32 æ¬¡å…ƒ**

### æ­£å‰‡åŒ–æˆ¦ç•¥

**éå­¦ç¿’é˜²æ­¢** (N=500 ã¯å°‘ãªã„ï¼):

- **Dropout**: å„å±¤ã§ 30%ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ç„¡åŠ¹åŒ–
- **Weight Decay**: L2 æ­£å‰‡åŒ– (Î»=1e-4)
- **Early Stopping**: Validation Loss ãŒ 3 ã‚¨ãƒãƒƒã‚¯æ”¹å–„ã—ãªã‘ã‚Œã°åœæ­¢
- **Data Augmentation**: Self-Play ã§è¿½åŠ ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

---

## ğŸ“š Behavioral Cloning (BC)

### äº‹å‰å­¦ç¿’æˆ¦ç•¥

**ç›®çš„**: å°‘ãªã„ãƒ‡ãƒ¼ã‚¿(N=500)ã‹ã‚‰æœ€å¤§é™ã®çŸ¥è­˜ã‚’æŠ½å‡º

**æ‰‹æ³•**:

1. **ä¸Šä½ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ­ã‚°åé›†**

   - Pokemon Showdown Ladder ä¸Šä½ 100 å
   - å…¬å¼å¤§ä¼šã®ãƒªãƒ—ãƒ¬ã‚¤ (Worlds, Nationals)
   - åˆè¨ˆ N=500 è©¦åˆ

2. **æ•™å¸«ã‚ã‚Šå­¦ç¿’**

   ```python
   Loss = CrossEntropy(Policy, Expert_Action) + MSE(Value, Outcome)
   ```

3. **å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«**
   - Epochs: 50
   - Batch Size: 32
   - Learning Rate: 1e-3 â†’ 1e-5 (Cosine Annealing)

**åŠ¹æœ**:

- NN ãŒã€Œãƒ—ãƒ­ã®ç›´æ„Ÿã€ã‚’ç²å¾—
- MCTS ã®åˆæœŸæ¢ç´¢æ–¹å‘ãŒè³¢ããªã‚‹
- Pure MCTS ã® 10 å€ã®åŠ¹ç‡ (100 rollouts â‰ˆ 1000 rollouts)

---

## ğŸŒ² AlphaZero MCTS

### UCB å¼ã®æ‹¡å¼µ

é€šå¸¸ã® UCB:

```
UCB = Q + c * sqrt(log(N_total) / N_action)
```

AlphaZero UCB (PUCT):

```
UCB = Q + c_puct * P * sqrt(N_total) / (1 + N_action)
```

- **Q**: å¹³å‡ä¾¡å€¤ (Exploitation)
- **P**: Policy Network ã®ç¢ºç‡ (Prior)
- **N**: è¨ªå•å›æ•° (Exploration)
- **c_puct**: æ¢ç´¢ä¿‚æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.0)

### æ¢ç´¢ãƒ•ãƒ­ãƒ¼

```
1. Selection: UCBå¼ã§æ¬¡ã®æ‰‹ã‚’é¸æŠ
    â†“
2. Expansion: Policyã§æœ‰æœ›ãªæ‰‹ã‚’å±•é–‹
    â†“
3. Evaluation: Value Networkã§è©•ä¾¡
    â†“
4. Backpropagation: çµ±è¨ˆã‚’æ›´æ–°
```

### Value Network ã«ã‚ˆã‚‹çŸ­ç¸®

é€šå¸¸ã® MCTS:

- ãƒ©ãƒ³ãƒ€ãƒ ãƒ—ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ â†’ çµ‚å±€ã¾ã§ (50 ã‚¿ãƒ¼ãƒ³)

AlphaZero MCTS:

- Value Network è©•ä¾¡ â†’ **1 ã‚¹ãƒ†ãƒƒãƒ—ã§å®Œäº†**
- æ¨è«–æ™‚é–“ãŒåŠ‡çš„ã«çŸ­ç¸®

---

## ğŸš€ å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º

### Phase 1: åŸºç›¤æ§‹ç¯‰ (Week 4) âœ…

- [x] `alphazero_strategist.py` ä½œæˆ
- [x] `PolicyValueNetwork` ã‚¯ãƒ©ã‚¹å®Ÿè£…
- [x] `AlphaZeroMCTS` ã‚¯ãƒ©ã‚¹å®Ÿè£…
- [x] `HybridStrategist` çµ±åˆ
- [ ] ãƒ€ãƒŸãƒ¼å®Ÿè£…ã§ãƒ†ã‚¹ãƒˆ

**æˆæœç‰©**:

- `predictor/player/alphazero_strategist.py` (600 è¡Œ)
- `hybrid_strategist.py` æ‹¡å¼µ (3 å±¤ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)

### Phase 2: NN å®Ÿè£… (Week 5-6)

- [ ] PyTorch å®Ÿè£…
  - [ ] BattleState â†’ ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«å¤‰æ›
  - [ ] Policy/Value Network å®šç¾©
  - [ ] Forward Pass å®Ÿè£…
- [ ] è¨“ç·´ãƒ«ãƒ¼ãƒ—å®Ÿè£…
  - [ ] BC Loss è¨ˆç®—
  - [ ] Optimizer è¨­å®š (Adam + Weight Decay)
  - [ ] Validation Split
- [ ] ãƒ¢ãƒ‡ãƒ«ä¿å­˜/èª­ã¿è¾¼ã¿

**æˆæœç‰©**:

- `models/policy_value.pt` (å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«)
- `scripts/train_bc.py` (BC è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ)

### Phase 3: ãƒ‡ãƒ¼ã‚¿åé›† (Week 7)

- [ ] Showdown Replay åé›†
  - [ ] ä¸Šä½ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ­ã‚° (N=500)
  - [ ] ãƒªãƒ—ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚µãƒ¼å®Ÿè£…
  - [ ] BattleState + Action ãƒšã‚¢ç”Ÿæˆ
- [ ] ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
  - [ ] ç„¡åŠ¹ãªè©¦åˆã‚’é™¤å¤–
  - [ ] Train/Val Split (80/20)

**æˆæœç‰©**:

- `data/expert_logs/` (N=500 è©¦åˆ)
- `scripts/fetch_expert_logs.py`

### Phase 4: BC è¨“ç·´ (Week 8)

- [ ] äº‹å‰å­¦ç¿’å®Ÿè¡Œ
  - [ ] 50 epochs è¨“ç·´
  - [ ] Validation Loss ç›£è¦–
  - [ ] ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
- [ ] æ€§èƒ½è©•ä¾¡
  - [ ] Policy Accuracy æ¸¬å®š
  - [ ] Value MSE æ¸¬å®š
  - [ ] ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§æ¤œè¨¼

**ç›®æ¨™**:

- Policy Accuracy > 30% (ãƒ©ãƒ³ãƒ€ãƒ : 6.25%)
- Value MSE < 0.1

### Phase 5: Self-Play (Week 9+)

- [ ] Self-Play å®Ÿè£…
  - [ ] 2 ã¤ã® AlphaZero ã‚’å¯¾æˆ¦
  - [ ] è»Œè·¡è¨˜éŒ²
  - [ ] ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã§å†å­¦ç¿’
- [ ] åå¾©æ”¹å–„
  - [ ] N å› Self-Play â†’ å†è¨“ç·´
  - [ ] æ€§èƒ½æ¸¬å®š

**ç›®æ¨™**:

- Self-Play 1000 è©¦åˆç”Ÿæˆ
- å‹ç‡äºˆæ¸¬ç²¾åº¦ +5%å‘ä¸Š

---

## ğŸ“Š æ€§èƒ½ç›®æ¨™

| æŒ‡æ¨™                | Phase 1<br>(Pure MCTS)  | Phase 4<br>(BC äº‹å‰å­¦ç¿’) | Phase 5<br>(Self-Play) |
| ------------------- | ----------------------- | ------------------------ | ---------------------- |
| **æ¨è«–æ™‚é–“**        | 100ms<br>(100 rollouts) | 80ms<br>(100 rollouts)   | 60ms<br>(100 rollouts) |
| **å‹ç‡äºˆæ¸¬ç²¾åº¦**    | 65%                     | 70%                      | 75%                    |
| **Policy Accuracy** | N/A                     | 30%                      | 40%                    |
| **ä¿¡é ¼åº¦**          | 90%                     | 95%                      | 98%                    |

---

## ğŸ”§ æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

### æ·±å±¤å­¦ç¿’

- **Framework**: PyTorch 2.0+
- **Optimizer**: Adam + Weight Decay
- **Scheduler**: CosineAnnealingLR
- **Regularization**: Dropout(0.3) + L2(1e-4)

### ãƒ‡ãƒ¼ã‚¿å‡¦ç†

- **Feature Engineering**: BattleState â†’ 512 æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«
- **Action Encoding**: æŠ€å â†’ ID å¤‰æ›
- **Target Encoding**: ã‚¹ãƒ­ãƒƒãƒˆç•ªå· (0-3)

### å®Ÿé¨“ç®¡ç†

- **Logging**: TensorBoard / Weights & Biases
- **Checkpointing**: å„ã‚¨ãƒãƒƒã‚¯ä¿å­˜
- **Version Control**: Git LFS for models

---

## ğŸ¯ æˆåŠŸåŸºæº–

### Phase 2 å®Œäº†æ™‚

- [ ] PyTorch å®Ÿè£…ãŒå‹•ä½œ
- [ ] ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´å®Œäº†
- [ ] ãƒ¢ãƒ‡ãƒ«ãŒä¿å­˜/èª­ã¿è¾¼ã¿å¯èƒ½

### Phase 4 å®Œäº†æ™‚

- [ ] BC äº‹å‰å­¦ç¿’å®Œäº†
- [ ] Policy Accuracy > 30%
- [ ] Streamlit UI ã§çµæœè¡¨ç¤º

### Phase 5 å®Œäº†æ™‚

- [ ] Self-Play 1000 è©¦åˆç”Ÿæˆ
- [ ] å‹ç‡äºˆæ¸¬ç²¾åº¦ > 75%
- [ ] æ¨è«–æ™‚é–“ < 100ms

---

## ğŸ“ è£œè¶³è³‡æ–™

### å‚è€ƒè«–æ–‡

1. **AlphaZero** (Silver et al., 2017)

   - Self-Play + MCTS + NN
   - å°†æ£‹ãƒ»å›²ç¢ãƒ»ãƒã‚§ã‚¹ã§äººé–“è¶…ãˆ

2. **AlphaGo Zero** (Silver et al., 2017)

   - ãƒ‡ãƒ¼ã‚¿ 0 ã‹ã‚‰å­¦ç¿’
   - Policy/Value Network

3. **MuZero** (Schrittwieser et al., 2020)
   - ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹å¼·åŒ–å­¦ç¿’
   - Atariãƒ»å›²ç¢ã§æˆåŠŸ

### VGC ç‰¹æœ‰ã®èª²é¡Œ

- **ãƒ€ãƒ–ãƒ«ãƒãƒˆãƒ«**: è¡Œå‹•ç©ºé–“ãŒè†¨å¤§ â†’ Factored Action Space
- **ä¸å®Œå…¨æƒ…å ±**: ç›¸æ‰‹ã®åŠªåŠ›å€¤ä¸æ˜ â†’ Detective Engine çµ±åˆ
- **ç¢ºç‡è¦ç´ **: æ€¥æ‰€ãƒ»è¿½åŠ åŠ¹æœ â†’ æœŸå¾…å€¤ã§è©•ä¾¡

---

## ğŸš€ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

**Phase 1 (ä»Šé€±)**:

1. ãƒ€ãƒŸãƒ¼å®Ÿè£…ã®ãƒ†ã‚¹ãƒˆ
2. Streamlit UI ã«çµ±åˆ
3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š

**Phase 2 æº–å‚™**:

1. PyTorch ç’°å¢ƒæ§‹ç¯‰
2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°è¨­è¨ˆ
3. ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆæº–å‚™

**è³ªå•äº‹é …**:

- N=500 ã¯ååˆ†ã‹ï¼Ÿ (ç†æƒ³ã¯ 5000+)
- GPU ç’°å¢ƒã¯ï¼Ÿ (M2 Mac / Colab)
- Self-Play ã®å„ªå…ˆåº¦ï¼Ÿ

---

**ä½œæˆè€…**: GitHub Copilot  
**ãƒ¬ãƒ“ãƒ¥ãƒ¼**: Pending
