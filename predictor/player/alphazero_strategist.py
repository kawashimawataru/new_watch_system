"""
AlphaZero-Style Strategist: Neural Network + MCTS Hybrid

AlphaZeroã®æˆ¦ç•¥ã‚’æ¡ç”¨ã—ãŸã€VGCï¼ˆãƒ€ãƒ–ãƒ«ãƒãƒˆãƒ«ï¼‰ç”¨ã®é«˜åº¦ãªæ„æ€æ±ºå®šã‚¨ãƒ³ã‚¸ãƒ³ã€‚

ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:
1. Policy Network: å„ãƒã‚±ãƒ¢ãƒ³ã®è¡Œå‹•ç¢ºç‡ã‚’äºˆæ¸¬ (Factored Action Space)
2. Value Network: ç¾åœ¨ã®ç›¤é¢ã®å‹ç‡ã‚’äºˆæ¸¬
3. MCTS: Policy/Valueã§èª˜å°ã•ã‚ŒãŸåŠ¹ç‡çš„ãªæ¢ç´¢

ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡åŒ–:
- Behavioral Cloning (BC): å°‘æ•°(N=500)ã®ä¸Šä½ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ­ã‚°ã§åˆæœŸå­¦ç¿’
- Regularization: Dropout + Weight Decay ã§éå­¦ç¿’ã‚’é˜²æ­¢
- Self-Play: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«åŒå£«ã§å¯¾æˆ¦ã—ã¦ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ

Usage:
    strategist = AlphaZeroStrategist(
        policy_value_model_path="models/policy_value.pt",
        mcts_rollouts=100,
        use_bc_pretraining=True
    )
    
    result = strategist.predict(battle_state)
    # => {
    #     "p1_win_rate": 0.73,
    #     "recommended_action": TurnAction(...),
    #     "policy_probs": {...},
    #     "value_estimate": 0.46
    # }

å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º: P1-4 (Week 4+)
å„ªå…ˆåº¦: HIGH ğŸ”¥
"""

from __future__ import annotations

import copy
import math
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from predictor.core.models import BattleState, ActionCandidate
from predictor.player.monte_carlo_strategist import Action, TurnAction, MonteCarloStrategist

try:
    import torch
    from predictor.player.policy_value_network_pytorch import (
        PolicyValueNet, BattleStateEncoder
    )
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False


@dataclass
class PolicyValueOutput:
    """
    Policy/Value Network ã®å‡ºåŠ›
    
    Attributes:
        policy_pokemon1: ãƒã‚±ãƒ¢ãƒ³1ã®è¡Œå‹•ç¢ºç‡åˆ†å¸ƒ (Factored)
        policy_pokemon2: ãƒã‚±ãƒ¢ãƒ³2ã®è¡Œå‹•ç¢ºç‡åˆ†å¸ƒ (Factored)
        value: ç¾åœ¨ã®ç›¤é¢è©•ä¾¡å€¤ (-1.0 ~ 1.0, P1è¦–ç‚¹)
        inference_time_ms: æ¨è«–æ™‚é–“
    """
    policy_pokemon1: Dict[str, float]  # {action_id: probability}
    policy_pokemon2: Dict[str, float]
    value: float  # -1.0 (P2å‹åˆ©ç¢ºå®Ÿ) ~ 1.0 (P1å‹åˆ©ç¢ºå®Ÿ)
    inference_time_ms: float


class PolicyValueNetwork:
    """
    Policy/Value Network
    
    æ§‹é€ :
    - Input: BattleStateã®ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ« (ç›¤é¢è¡¨ç¾)
    - Hidden: è¤‡æ•°ã®å…¨çµåˆå±¤ + Dropout
    - Output:
      * Policy Head 1: ãƒã‚±ãƒ¢ãƒ³1ã®è¡Œå‹•ç¢ºç‡ (softmax)
      * Policy Head 2: ãƒã‚±ãƒ¢ãƒ³2ã®è¡Œå‹•ç¢ºç‡ (softmax)
      * Value Head: å‹ç‡è©•ä¾¡å€¤ (tanh, -1~1)
    
    Phase 1å®Ÿè£…: ç°¡æ˜“ç‰ˆ (ãƒ©ãƒ³ãƒ€ãƒ å‡ºåŠ›)
    Phase 2å®Ÿè£…: PyTorch/TensorFlowã§æœ¬æ ¼å®Ÿè£…
    """
    
    def __init__(
        self,
        model_path: Optional[Path] = None,
        use_bc_pretraining: bool = True,
        dropout_rate: float = 0.3,
        weight_decay: float = 1e-4
    ):
        self.model_path = model_path
        self.use_bc_pretraining = use_bc_pretraining
        self.dropout_rate = dropout_rate
        self.weight_decay = weight_decay
        
        self.model = None
        self.device = "mps" if TORCH_AVAILABLE and torch.backends.mps.is_available() else "cpu"
        
        if TORCH_AVAILABLE and model_path and Path(model_path).exists():
            self._load_model(model_path)
        else:
            if TORCH_AVAILABLE:
                self._initialize_random_model()
            else:
                print("âš ï¸  PyTorch not available, using dummy model")
    
    def predict(self, battle_state: BattleState) -> PolicyValueOutput:
        start_time = time.perf_counter()
        
        if not TORCH_AVAILABLE or self.model is None:
            return self._dummy_predict(battle_state, start_time)
        
        # Encode state
        state_features = BattleStateEncoder.encode(battle_state)
        state_tensor = torch.tensor(
            state_features, dtype=torch.float32
        ).unsqueeze(0).to(self.device)
        
        # Forward pass
        self.model.eval()
        with torch.no_grad():
            policy1_logits, policy2_logits, value = self.model(state_tensor)
            
            policy1_probs = torch.softmax(policy1_logits, dim=1)[0]
            policy2_probs = torch.softmax(policy2_logits, dim=1)[0]
            value_scalar = value[0, 0].item()
        
        # Convert to dict
        policy_p1 = {
            f"action_{i}": prob.item()
            for i, prob in enumerate(policy1_probs)
        }
        policy_p2 = {
            f"action_{i}": prob.item()
            for i, prob in enumerate(policy2_probs)
        }
        
        elapsed_ms = (time.perf_counter() - start_time) * 1000
        
        return PolicyValueOutput(
            policy_pokemon1=policy_p1,
            policy_pokemon2=policy_p2,
            value=value_scalar,
            inference_time_ms=elapsed_ms
        )
    
    def _dummy_predict(self, battle_state: BattleState, start_time: float) -> PolicyValueOutput:
        legal_actions_p1 = self._get_legal_actions_for_pokemon(battle_state, player="A", slot=0)
        legal_actions_p2 = self._get_legal_actions_for_pokemon(battle_state, player="A", slot=1)
        
        policy_p1 = {action: 1.0 / len(legal_actions_p1) for action in legal_actions_p1} if legal_actions_p1 else {}
        policy_p2 = {action: 1.0 / len(legal_actions_p2) for action in legal_actions_p2} if legal_actions_p2 else {}
        
        value = np.random.uniform(-0.5, 0.5)
        
        elapsed_ms = (time.perf_counter() - start_time) * 1000
        
        return PolicyValueOutput(
            policy_pokemon1=policy_p1,
            policy_pokemon2=policy_p2,
            value=value,
            inference_time_ms=elapsed_ms
        )
    
    def train_behavioral_cloning(
        self,
        expert_trajectories: List[Dict[str, Any]],
        epochs: int = 50,
        batch_size: int = 32,
        learning_rate: float = 1e-3
    ) -> Dict[str, float]:
        """
        Behavioral Cloning (BC) ã«ã‚ˆã‚‹äº‹å‰å­¦ç¿’
        
        ä¸Šä½ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ­ã‚° (N=500è©¦åˆ) ã‹ã‚‰ã€
        ã€Œã“ã®ç›¤é¢ã§ãƒ—ãƒ­ã¯ã©ã†æ‰“ã¤ã‹ã€ã‚’å­¦ç¿’ã™ã‚‹ã€‚
        
        Args:
            expert_trajectories: [{"state": BattleState, "action": TurnAction}, ...]
            epochs: å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            learning_rate: å­¦ç¿’ç‡
            
        Returns:
            {"loss": final_loss, "accuracy": final_accuracy}
        """
        print(f"ğŸ“ Behavioral Cloning é–‹å§‹: {len(expert_trajectories)} trajectories")
        
        # Phase 2å®Ÿè£…:
        # 1. expert_trajectories ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
        # 2. Cross-Entropy Loss ã§ Policy ã‚’å­¦ç¿’
        # 3. MSE Loss ã§ Value ã‚’å­¦ç¿’
        # 4. Dropout + Weight Decay ã§æ­£å‰‡åŒ–
        
        # Phase 1: ãƒ€ãƒŸãƒ¼å®Ÿè£…
        return {
            "loss": 0.0,
            "accuracy": 0.0
        }
    
    def _load_model(self, model_path: Path):
        print(f"ğŸ“¥ Loading: {model_path}")
        if TORCH_AVAILABLE:
            self.model = PolicyValueNet().to(self.device)
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint["model_state_dict"])
            self.model.eval()
    
    def _initialize_random_model(self):
        print("ğŸ² Random init")
        if TORCH_AVAILABLE:
            self.model = PolicyValueNet().to(self.device)
    
    def _get_legal_actions_for_pokemon(
        self,
        battle_state: BattleState,
        player: str,
        slot: int
    ) -> List[str]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒã‚±ãƒ¢ãƒ³ã®åˆæ³•æ‰‹ãƒªã‚¹ãƒˆã‚’å–å¾—
        
        Args:
            battle_state: å¯¾æˆ¦çŠ¶æ…‹
            player: "A" or "B"
            slot: ãƒã‚±ãƒ¢ãƒ³ã‚¹ãƒ­ãƒƒãƒˆ (0 or 1)
            
        Returns:
            ["move_Moonblast_target2", "move_ShadowBall_target2", ...]
        """
        # Phase 1: ç°¡æ˜“å®Ÿè£…
        legal_actions = battle_state.legal_actions.get(player, [])
        
        # ã‚¹ãƒ­ãƒƒãƒˆæŒ‡å®šã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (Phase 2ã§æ”¹å–„)
        action_strings = []
        for action in legal_actions:
            if hasattr(action, 'slot') and action.slot == slot:
                action_id = f"move_{action.move}_target{action.target or 0}"
                action_strings.append(action_id)
        
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
        if not action_strings:
            action_strings = [f"move_{i}" for i in range(4)]
        
        return action_strings


class AlphaZeroMCTS:
    """
    Policy/Value Network ã§èª˜å°ã•ã‚Œã‚‹ MCTS
    
    é€šå¸¸ã®MCTSã¨ã®é•ã„:
    - Policy Networkã®äºˆæ¸¬ç¢ºç‡ã§æ¢ç´¢ã‚’åã‚‰ã›ã‚‹ (UCBå¼ã«çµ„ã¿è¾¼ã‚€)
    - Value Networkã®è©•ä¾¡å€¤ã§ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’çŸ­ç¸®
    - æ¢ç´¢åŠ¹ç‡ãŒåŠ‡çš„ã«å‘ä¸Š (100 rollouts ã§ 1000 rolloutsç›¸å½“ã®ç²¾åº¦)
    """
    
    def __init__(
        self,
        policy_value_network: PolicyValueNetwork,
        n_rollouts: int = 100,
        c_puct: float = 1.0,
        temperature: float = 1.0
    ):
        """
        Args:
            policy_value_network: Policy/Value Network
            n_rollouts: MCTS rolloutå›æ•° (NNã‚ã‚Š â†’ å°‘ãªãã¦ã‚‚OK)
            c_puct: UCBã®æ¢ç´¢ä¿‚æ•° (å¤§ãã„ã»ã©æ¢ç´¢é‡è¦–)
            temperature: è¡Œå‹•é¸æŠã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ (0=è²ªæ¬², 1=ç¢ºç‡çš„)
        """
        self.policy_value_network = policy_value_network
        self.n_rollouts = n_rollouts
        self.c_puct = c_puct
        self.temperature = temperature
        
        # MCTSçµ±è¨ˆ: {state_hash: {action: {"N": visit_count, "W": win_count, "Q": mean_value, "P": prior_prob}}}
        self.stats = {}
    
    def search(self, battle_state: BattleState) -> Tuple[TurnAction, float]:
        """
        MCTSæ¢ç´¢ã‚’å®Ÿè¡Œã—ã€æœ€é©ãªè¡Œå‹•ã‚’è¿”ã™
        
        Args:
            battle_state: ç¾åœ¨ã®å¯¾æˆ¦çŠ¶æ…‹
            
        Returns:
            (optimal_action, win_rate)
        """
        # Policy/Value æ¨è«–
        pv_output = self.policy_value_network.predict(battle_state)
        
        # çŠ¶æ…‹ã®ãƒãƒƒã‚·ãƒ¥åŒ–
        state_hash = self._hash_state(battle_state)
        
        # åˆå›è¨ªå•: Policy ã‚’ Prior ã¨ã—ã¦ç™»éŒ²
        if state_hash not in self.stats:
            self.stats[state_hash] = {}
            
            # Factored Action Space: 2ã¤ã®ãƒã‚±ãƒ¢ãƒ³ã®è¡Œå‹•ã‚’çµ„ã¿åˆã‚ã›ã‚‹
            for action1_id, prob1 in pv_output.policy_pokemon1.items():
                for action2_id, prob2 in pv_output.policy_pokemon2.items():
                    combined_action_id = f"{action1_id}|{action2_id}"
                    prior_prob = prob1 * prob2  # ç‹¬ç«‹ã¨ã—ã¦æ‰±ã†
                    
                    self.stats[state_hash][combined_action_id] = {
                        "N": 0,  # è¨ªå•å›æ•°
                        "W": 0.0,  # å‹åˆ©æ•°
                        "Q": 0.0,  # å¹³å‡ä¾¡å€¤
                        "P": prior_prob  # Priorç¢ºç‡ (Policy)
                    }
        
        # n_rollouts å›ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        for _ in range(self.n_rollouts):
            self._simulate(battle_state, state_hash)
        
        # æœ€ã‚‚è¨ªå•å›æ•°ãŒå¤šã„è¡Œå‹•ã‚’é¸æŠ
        best_action_id = max(
            self.stats[state_hash],
            key=lambda a: self.stats[state_hash][a]["N"]
        )
        
        best_action = self._decode_action(best_action_id, battle_state)
        win_rate = self.stats[state_hash][best_action_id]["Q"]
        
        return best_action, win_rate
    
    def _simulate(self, battle_state: BattleState, state_hash: str) -> float:
        """
        1å›ã®MCTSã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        
        Returns:
            è©•ä¾¡å€¤ (-1.0 ~ 1.0)
        """
        # UCBå¼ã§æ¬¡ã®è¡Œå‹•ã‚’é¸æŠ
        action_id = self._select_action_ucb(state_hash)
        
        # è¡Œå‹•ã‚’é©ç”¨ (ç°¡æ˜“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³)
        # Phase 2: å®Ÿéš›ã®ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³çµ±åˆ
        next_state = copy.deepcopy(battle_state)
        
        # Value Networkã§è©•ä¾¡
        pv_output = self.policy_value_network.predict(next_state)
        value = pv_output.value
        
        # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
        self.stats[state_hash][action_id]["N"] += 1
        self.stats[state_hash][action_id]["W"] += value
        self.stats[state_hash][action_id]["Q"] = (
            self.stats[state_hash][action_id]["W"] / 
            self.stats[state_hash][action_id]["N"]
        )
        
        return value
    
    def _select_action_ucb(self, state_hash: str) -> str:
        """
        UCB (Upper Confidence Bound) å¼ã§è¡Œå‹•é¸æŠ
        
        UCB = Q + c_puct * P * sqrt(N_total) / (1 + N_action)
        
        - Q: å¹³å‡ä¾¡å€¤ (exploitation)
        - P: Priorç¢ºç‡ (Policyèª˜å°)
        - N: è¨ªå•å›æ•° (exploration)
        """
        total_visits = sum(
            self.stats[state_hash][a]["N"]
            for a in self.stats[state_hash]
        )
        
        best_action = None
        best_ucb = -float("inf")
        
        for action_id, stats in self.stats[state_hash].items():
            q_value = stats["Q"]
            prior = stats["P"]
            visits = stats["N"]
            
            ucb = q_value + self.c_puct * prior * math.sqrt(total_visits) / (1 + visits)
            
            if ucb > best_ucb:
                best_ucb = ucb
                best_action = action_id
        
        return best_action
    
    def _hash_state(self, battle_state: BattleState) -> str:
        """çŠ¶æ…‹ã‚’ãƒãƒƒã‚·ãƒ¥åŒ– (ç°¡æ˜“å®Ÿè£…)"""
        # Phase 2: ã‚ˆã‚Šå³å¯†ãªãƒãƒƒã‚·ãƒ¥é–¢æ•°
        return f"turn_{battle_state.turn}"
    
    def _decode_action(self, action_id: str, battle_state: BattleState) -> TurnAction:
        """
        action_id ("move_Moonblast_target2|move_FlareBlitz_target3") ã‚’
        TurnAction ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        """
        # Phase 2å®Ÿè£…
        return TurnAction(
            player_a_actions=[
                Action(type="move", pokemon_slot=0, move_name="Moonblast", target_slot=2)
            ],
            player_b_actions=[
                Action(type="move", pokemon_slot=0, move_name="tackle", target_slot=0)
            ]
        )


class AlphaZeroStrategist:
    """
    AlphaZero-Style Strategist
    
    çµ±åˆã‚·ã‚¹ãƒ†ãƒ :
    - Policy/Value Network (BCäº‹å‰å­¦ç¿’)
    - MCTS (NNèª˜å°æ¢ç´¢)
    - Self-Play (ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ)
    """
    
    def __init__(
        self,
        policy_value_model_path: Optional[Path] = None,
        mcts_rollouts: int = 100,
        use_bc_pretraining: bool = True,
        mcts_c_puct: float = 1.0
    ):
        """
        Args:
            policy_value_model_path: Policy/Value Networkã®ãƒ‘ã‚¹
            mcts_rollouts: MCTS rolloutå›æ•°
            use_bc_pretraining: BCäº‹å‰å­¦ç¿’ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            mcts_c_puct: MCTSã®æ¢ç´¢ä¿‚æ•°
        """
        # Policy/Value Network åˆæœŸåŒ–
        self.policy_value_network = PolicyValueNetwork(
            model_path=policy_value_model_path,
            use_bc_pretraining=use_bc_pretraining
        )
        
        # MCTS åˆæœŸåŒ–
        self.mcts = AlphaZeroMCTS(
            policy_value_network=self.policy_value_network,
            n_rollouts=mcts_rollouts,
            c_puct=mcts_c_puct
        )
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç´”ç²‹MCTS (Phase 1)
        self.fallback_mcts = MonteCarloStrategist(
            n_rollouts=mcts_rollouts,
            max_turns=50
        )
    
    def predict(
        self,
        battle_state: BattleState,
        use_fallback: bool = False
    ) -> Dict[str, Any]:
        """
        å‹ç‡äºˆæ¸¬ + æœ€é©è¡Œå‹•é¸æŠ
        
        Args:
            battle_state: ç¾åœ¨ã®å¯¾æˆ¦çŠ¶æ…‹
            use_fallback: Phase 1ã§ã¯ç´”ç²‹MCTSã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
        Returns:
            {
                "p1_win_rate": float,
                "recommended_action": TurnAction,
                "policy_probs": Dict,
                "value_estimate": float,
                "inference_time_ms": float
            }
        """
        start_time = time.perf_counter()
        
        if use_fallback:
            # Phase 1: ç´”ç²‹MCTSã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            result = self.fallback_mcts.predict_win_rate(battle_state)
            return {
                "p1_win_rate": result["player_a_win_rate"],
                "recommended_action": result["optimal_action"],
                "policy_probs": {},
                "value_estimate": result["player_a_win_rate"] * 2 - 1,  # 0~1 â†’ -1~1
                "inference_time_ms": (time.perf_counter() - start_time) * 1000
            }
        
        # Phase 2: AlphaZero-Style Search
        optimal_action, win_rate = self.mcts.search(battle_state)
        pv_output = self.policy_value_network.predict(battle_state)
        
        elapsed_ms = (time.perf_counter() - start_time) * 1000
        
        return {
            "p1_win_rate": (win_rate + 1.0) / 2.0,  # -1~1 â†’ 0~1
            "recommended_action": optimal_action,
            "policy_probs": {
                "pokemon1": pv_output.policy_pokemon1,
                "pokemon2": pv_output.policy_pokemon2
            },
            "value_estimate": pv_output.value,
            "inference_time_ms": elapsed_ms
        }
    
    def train_from_expert_logs(
        self,
        expert_log_dir: Path,
        epochs: int = 50
    ):
        """
        ä¸Šä½ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ­ã‚°ã‹ã‚‰ Behavioral Cloning
        
        Args:
            expert_log_dir: ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            epochs: å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°
        """
        print(f"ğŸ“ Behavioral Cloning è¨“ç·´é–‹å§‹: {expert_log_dir}")
        
        # Phase 2å®Ÿè£…:
        # 1. expert_log_dir ã‹ã‚‰ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        # 2. BattleState + TurnAction ã®ãƒšã‚¢ã«å¤‰æ›
        # 3. policy_value_network.train_behavioral_cloning() ã‚’å®Ÿè¡Œ
        
        # Phase 1: ãƒ€ãƒŸãƒ¼
        pass
    
    def self_play(
        self,
        n_games: int = 100,
        save_trajectories: bool = True
    ) -> List[Dict]:
        """
        Self-Play ã§ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
        
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«åŒå£«ã‚’å¯¾æˆ¦ã•ã›ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        
        Args:
            n_games: å¯¾æˆ¦å›æ•°
            save_trajectories: è»Œè·¡ã‚’ä¿å­˜ã™ã‚‹ã‹
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸå¯¾æˆ¦ãƒ­ã‚°
        """
        print(f"ğŸ® Self-Play é–‹å§‹: {n_games} games")
        
        # Phase 3å®Ÿè£…:
        # 1. 2ã¤ã® AlphaZeroStrategist ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å¯¾æˆ¦ã•ã›ã‚‹
        # 2. å„ã‚¿ãƒ¼ãƒ³ã® (state, action, outcome) ã‚’è¨˜éŒ²
        # 3. ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã§å†å­¦ç¿’
        
        return []
