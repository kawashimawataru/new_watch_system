"""
LLMClient - LLMå‘¼ã³å‡ºã—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

PokÃ©Champå‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®LLMçµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
å€™è£œç”Ÿæˆ/ç›¸æ‰‹ãƒ¢ãƒ‡ãƒ«/ä¾¡å€¤æ¨å®š/èª¬æ˜ç”Ÿæˆã‚’æ‹…å½“ã€‚

References:
- PokÃ©Champ: https://arxiv.org/abs/2503.04094
- PokeLLMon: https://arxiv.org/abs/2402.01118 (KAG)
"""

from __future__ import annotations

import json
import os
import re
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple

try:
    from poke_env.environment.double_battle import DoubleBattle
except ImportError:
    try:
        from poke_env.battle import DoubleBattle
    except ImportError:
        DoubleBattle = None


# ============================================================================
# è¨­å®š
# ============================================================================

@dataclass
class LLMConfig:
    """LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è¨­å®š"""
    provider: str = "openai"              # "openai", "anthropic", "google"
    model: str = "gpt-4o-mini"            # ãƒ¢ãƒ‡ãƒ«å
    temperature: float = 0.3              # ç”Ÿæˆæ¸©åº¦
    max_tokens: int = 1024                # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    timeout: float = 10.0                 # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
    api_key: Optional[str] = None         # APIã‚­ãƒ¼ï¼ˆNoneãªã‚‰ç’°å¢ƒå¤‰æ•°ï¼‰


# ============================================================================
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
# ============================================================================

CANDIDATE_GENERATION_PROMPT = """ã‚ãªãŸã¯ãƒã‚±ãƒ¢ãƒ³VGCï¼ˆãƒ€ãƒ–ãƒ«ãƒãƒˆãƒ«ï¼‰ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®å±€é¢ã§ã€æœ‰åŠ¹ãªå€™è£œæ‰‹ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

## å±€é¢æƒ…å ±
{battle_summary}

## åˆæ³•ãªè¡Œå‹•ä¸€è¦§
{action_list}

## ã‚¿ã‚¹ã‚¯
å„è¡Œå‹•ã‚’è©•ä¾¡ã—ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
ã‚¹ã‚³ã‚¢ã¯0.0ã€œ1.0ã§ã€1.0ãŒæœ€ã‚‚æœ‰åŠ¹ãªè¡Œå‹•ã§ã™ã€‚
ä¸Šä½10å€‹ã®è¡Œå‹•ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚

```json
[
  {{"action_id": "A1", "score": 0.85, "tags": ["ko", "spread"]}},
  {{"action_id": "A2", "score": 0.70, "tags": ["speed", "setup"]}}
]
```

é‡è¦ãƒ«ãƒ¼ãƒ«:
- action_id ã¯å¿…ãšä¸Šè¨˜ä¸€è¦§ã«ã‚ã‚‹IDã‚’ä½¿ç”¨
- tags ã¯ä»¥ä¸‹ã‹ã‚‰é¸æŠ: ko, spread, speed, setup, protect, switch, redirection, tera
- JSONã®ã¿ã‚’è¿”ã™ï¼ˆèª¬æ˜ä¸è¦ï¼‰
"""

OPPONENT_MODELING_PROMPT = """ã‚ãªãŸã¯ãƒã‚±ãƒ¢ãƒ³VGCï¼ˆãƒ€ãƒ–ãƒ«ãƒãƒˆãƒ«ï¼‰ã®å°‚é–€å®¶ã§ã™ã€‚
ç›¸æ‰‹ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¦–ç‚¹ã§ã€ã©ã®è¡Œå‹•ã‚’é¸ã³ãã†ã‹ã‚’äºˆæ¸¬ã—ã¦ãã ã•ã„ã€‚

## å±€é¢æƒ…å ±ï¼ˆç›¸æ‰‹è¦–ç‚¹ï¼‰
{battle_summary}

## ç›¸æ‰‹ã®åˆæ³•ãªè¡Œå‹•ä¸€è¦§
{action_list}

## ã‚¿ã‚¹ã‚¯
ç›¸æ‰‹ãŒã©ã®è¡Œå‹•ã‚’é¸ã¶ã‹ç¢ºç‡åˆ†å¸ƒã‚’äºˆæ¸¬ã—ã¦ãã ã•ã„ã€‚
pã¯ç¢ºç‡ï¼ˆåˆè¨ˆ1.0ï¼‰ã€rationale_tags ã¯è¡Œå‹•ç†ç”±ã‚’ç¤ºã™ã‚¿ã‚°ã§ã™ã€‚

```json
[
  {{"action_id": "B1", "p": 0.40, "rationale_tags": ["aggressive", "ko_threat"]}},
  {{"action_id": "B2", "p": 0.30, "rationale_tags": ["safe", "protect"]}}
]
```

é‡è¦ãƒ«ãƒ¼ãƒ«:
- action_id ã¯å¿…ãšä¸Šè¨˜ä¸€è¦§ã«ã‚ã‚‹IDã‚’ä½¿ç”¨
- pã®åˆè¨ˆãŒ1.0ã«ãªã‚‹ã‚ˆã†ã«
- rationale_tags: aggressive, defensive, safe, risky, ko_threat, setup, pivot
"""

VALUE_ESTIMATION_PROMPT = """ã‚ãªãŸã¯ãƒã‚±ãƒ¢ãƒ³VGCï¼ˆãƒ€ãƒ–ãƒ«ãƒãƒˆãƒ«ï¼‰ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®å±€é¢ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

## å±€é¢æƒ…å ±
{battle_summary}

## ã‚¿ã‚¹ã‚¯
è‡ªåˆ†ï¼ˆP1ï¼‰è¦–ç‚¹ã§ã®æœ‰åˆ©åº¦ã‚’-1.0ã€œ+1.0ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
+1.0ã¯ç¢ºå®Ÿå‹åˆ©ã€-1.0ã¯ç¢ºå®Ÿæ•—åŒ—ã€0.0ã¯äº”åˆ†ã§ã™ã€‚

```json
{{"value": 0.3, "rationale_tags": ["hp_advantage", "speed_control"]}}
```

rationale_tags: hp_advantage, hp_disadvantage, speed_control, tera_advantage, momentum, numbers_advantage
"""

EXPLANATION_PROMPT = """ã‚ãªãŸã¯ãƒã‚±ãƒ¢ãƒ³VGCï¼ˆãƒ€ãƒ–ãƒ«ãƒãƒˆãƒ«ï¼‰ã®å®Ÿæ³è€…ã§ã™ã€‚
ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€çŠ¶æ³ã‚’åˆ†ã‹ã‚Šã‚„ã™ãè§£èª¬ã—ã¦ãã ã•ã„ã€‚

## æ ¹æ‹ æƒ…å ±
{anchors}

## ã‚¿ã‚¹ã‚¯
ä¸Šè¨˜ã®æ ¹æ‹ ã‚’åŸºã«ã€è¦³æˆ¦è€…å‘ã‘ã«1-2æ–‡ã§çŠ¶æ³ã‚’è§£èª¬ã—ã¦ãã ã•ã„ã€‚
- å°‚é–€ç”¨èªã¯é¿ã‘ã¦åˆ†ã‹ã‚Šã‚„ã™ã
- ã€Œã€œã§ã™ã€ã€Œã€œã¾ã™ã€èª¿ã§
- 50æ–‡å­—ä»¥å†…

å›ç­”ï¼ˆæ—¥æœ¬èªã®ã¿ï¼‰:
"""


# ============================================================================
# LLMClient
# ============================================================================

class LLMClient:
    """
    LLMå‘¼ã³å‡ºã—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    
    - generate_candidates: å€™è£œç”Ÿæˆ
    - model_opponent: ç›¸æ‰‹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
    - evaluate_state: ä¾¡å€¤æ¨å®š
    - generate_explanation: èª¬æ˜ç”Ÿæˆï¼ˆKAGï¼‰
    """
    
    def __init__(self, config: Optional[LLMConfig] = None):
        self.config = config or LLMConfig()
        self._client = None
        self._initialized = False
    
    def _ensure_initialized(self):
        """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        if self._initialized:
            return
        
        api_key = self.config.api_key or os.environ.get("OPENAI_API_KEY")
        
        if self.config.provider == "openai" and api_key:
            try:
                import openai
                self._client = openai.OpenAI(api_key=api_key)
                self._initialized = True
            except ImportError:
                print("âš ï¸ openai package not installed")
        elif self.config.provider == "anthropic":
            try:
                import anthropic
                self._client = anthropic.Anthropic(
                    api_key=api_key or os.environ.get("ANTHROPIC_API_KEY")
                )
                self._initialized = True
            except ImportError:
                print("âš ï¸ anthropic package not installed")
    
    def _call_llm(self, prompt: str) -> Optional[str]:
        """LLMã‚’å‘¼ã³å‡ºã—"""
        self._ensure_initialized()
        
        if not self._client:
            return None
        
        try:
            print(f"  ğŸ¤– LLMå‘¼ã³å‡ºã—ä¸­... (model: {self.config.model})")
            
            if self.config.provider == "openai":
                response = self._client.chat.completions.create(
                    model=self.config.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.config.temperature,
                    max_tokens=self.config.max_tokens,
                )
                result = response.choices[0].message.content
                # å®Œå…¨ãªLLMå¿œç­”ã‚’è¡¨ç¤º
                print(f"  âœ… LLMå¿œç­”å—ä¿¡ï¼ˆ{len(result)}æ–‡å­—ï¼‰:")
                print("  " + "-" * 50)
                for line in result.split("\n"):
                    print(f"  {line}")
                print("  " + "-" * 50)
                return result
            
            elif self.config.provider == "anthropic":
                response = self._client.messages.create(
                    model=self.config.model,
                    max_tokens=self.config.max_tokens,
                    messages=[{"role": "user", "content": prompt}],
                )
                result = response.content[0].text
                print(f"  âœ… LLMå¿œç­”: {result[:100]}..." if len(result) > 100 else f"  âœ… LLMå¿œç­”: {result}")
                return result
            
        except Exception as e:
            print(f"  âš ï¸ LLM call failed: {e}")
            return None
    
    def _extract_json(self, text: str) -> Any:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰JSONã‚’æŠ½å‡º"""
        if not text:
            return None
        
        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®JSONã‚’æ¢ã™
        json_match = re.search(r'```(?:json)?\s*([\s\S]*?)```', text)
        if json_match:
            text = json_match.group(1)
        
        try:
            return json.loads(text.strip())
        except json.JSONDecodeError:
            return None
    
    # ========================================================================
    # å€™è£œç”Ÿæˆ
    # ========================================================================
    
    async def generate_candidates(
        self,
        battle_summary: str,
        action_list: str,
    ) -> List[Dict]:
        """
        å€™è£œç”Ÿæˆç”¨LLMå‘¼ã³å‡ºã—
        
        Returns:
            [{action_id, score, tags}]
        """
        prompt = CANDIDATE_GENERATION_PROMPT.format(
            battle_summary=battle_summary,
            action_list=action_list,
        )
        
        response = self._call_llm(prompt)
        result = self._extract_json(response)
        
        if isinstance(result, list):
            return result
        return []
    
    def generate_candidates_sync(
        self,
        battle_summary: str,
        action_list: str,
    ) -> List[Dict]:
        """åŒæœŸç‰ˆ"""
        prompt = CANDIDATE_GENERATION_PROMPT.format(
            battle_summary=battle_summary,
            action_list=action_list,
        )
        
        response = self._call_llm(prompt)
        result = self._extract_json(response)
        
        if isinstance(result, list):
            return result
        return []
    
    # ========================================================================
    # ç›¸æ‰‹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
    # ========================================================================
    
    async def model_opponent(
        self,
        battle_summary: str,
        action_list: str,
    ) -> List[Dict]:
        """
        ç›¸æ‰‹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ç”¨LLMå‘¼ã³å‡ºã—
        
        Returns:
            [{action_id, p, rationale_tags}]
        """
        prompt = OPPONENT_MODELING_PROMPT.format(
            battle_summary=battle_summary,
            action_list=action_list,
        )
        
        response = self._call_llm(prompt)
        result = self._extract_json(response)
        
        if isinstance(result, list):
            return result
        return []
    
    def model_opponent_sync(
        self,
        battle_summary: str,
        action_list: str,
    ) -> List[Dict]:
        """åŒæœŸç‰ˆ"""
        prompt = OPPONENT_MODELING_PROMPT.format(
            battle_summary=battle_summary,
            action_list=action_list,
        )
        
        response = self._call_llm(prompt)
        result = self._extract_json(response)
        
        if isinstance(result, list):
            return result
        return []
    
    # ========================================================================
    # ä¾¡å€¤æ¨å®š
    # ========================================================================
    
    async def evaluate_state(
        self,
        battle_summary: str,
    ) -> Tuple[float, List[str]]:
        """
        ä¾¡å€¤æ¨å®šç”¨LLMå‘¼ã³å‡ºã—
        
        Returns:
            (value, rationale_tags)
        """
        prompt = VALUE_ESTIMATION_PROMPT.format(
            battle_summary=battle_summary,
        )
        
        response = self._call_llm(prompt)
        result = self._extract_json(response)
        
        if isinstance(result, dict):
            value = result.get("value", 0.0)
            tags = result.get("rationale_tags", [])
            return float(value), tags
        
        return 0.0, []
    
    def evaluate_state_sync(
        self,
        battle_summary: str,
    ) -> Tuple[float, List[str]]:
        """åŒæœŸç‰ˆ"""
        prompt = VALUE_ESTIMATION_PROMPT.format(
            battle_summary=battle_summary,
        )
        
        response = self._call_llm(prompt)
        result = self._extract_json(response)
        
        if isinstance(result, dict):
            value = result.get("value", 0.0)
            tags = result.get("rationale_tags", [])
            return float(value), tags
        
        return 0.0, []
    
    # ========================================================================
    # èª¬æ˜ç”Ÿæˆï¼ˆKAGï¼‰
    # ========================================================================
    
    async def generate_explanation(
        self,
        anchors: List[str],
    ) -> str:
        """
        èª¬æ˜ç”Ÿæˆç”¨LLMå‘¼ã³å‡ºã—ï¼ˆKAGï¼‰
        
        Returns:
            æ—¥æœ¬èªã®çŸ­ã„èª¬æ˜æ–‡
        """
        anchors_text = "\n".join(f"- {a}" for a in anchors)
        
        prompt = EXPLANATION_PROMPT.format(
            anchors=anchors_text,
        )
        
        response = self._call_llm(prompt)
        
        if response:
            # ä½™åˆ†ãªå¼•ç”¨ç¬¦ã‚„æ”¹è¡Œã‚’é™¤å»
            return response.strip().strip('"').strip()
        
        return ""
    
    def generate_explanation_sync(
        self,
        anchors: List[str],
    ) -> str:
        """åŒæœŸç‰ˆ"""
        anchors_text = "\n".join(f"- {a}" for a in anchors)
        
        prompt = EXPLANATION_PROMPT.format(
            anchors=anchors_text,
        )
        
        response = self._call_llm(prompt)
        
        if response:
            return response.strip().strip('"').strip()
        
        return ""


# ============================================================================
# ãƒãƒˆãƒ«è¦ç´„ç”Ÿæˆ
# ============================================================================

def summarize_battle(battle: DoubleBattle, side: str = "self") -> str:
    """ãƒãƒˆãƒ«çŠ¶æ…‹ã‚’è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
    lines = []
    lines.append(f"ã‚¿ãƒ¼ãƒ³: {battle.turn}")
    
    # è‡ªåˆ†ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–
    self_active = []
    for p in battle.active_pokemon[:2]:
        if p and not p.fainted:
            hp_pct = int(p.current_hp_fraction * 100)
            status = f"/{p.status.name}" if p.status else ""
            self_active.append(f"{p.species}(HP{hp_pct}%{status})")
    lines.append(f"è‡ªåˆ†: {', '.join(self_active) or 'ãªã—'}")
    
    # ç›¸æ‰‹ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–
    opp_active = []
    for p in battle.opponent_active_pokemon[:2]:
        if p and not p.fainted:
            hp_pct = int(p.current_hp_fraction * 100)
            status = f"/{p.status.name}" if p.status else ""
            opp_active.append(f"{p.species}(HP{hp_pct}%{status})")
    lines.append(f"ç›¸æ‰‹: {', '.join(opp_active) or 'ãªã—'}")
    
    # æ®‹æ•°
    self_remaining = sum(1 for p in battle.team.values() if p and not p.fainted)
    opp_remaining = sum(1 for p in battle.opponent_team.values() if p and not p.fainted)
    lines.append(f"æ®‹æ•°: è‡ªåˆ†{self_remaining} vs ç›¸æ‰‹{opp_remaining}")
    
    return "\n".join(lines)


def format_action_list(actions: List[Any], prefix: str = "A") -> str:
    """è¡Œå‹•ãƒªã‚¹ãƒˆã‚’æ–‡å­—åˆ—ã«å¤‰æ›"""
    lines = []
    for i, action in enumerate(actions):
        action_id = f"{prefix}{i+1}"
        lines.append(f"- {action_id}: {action}")
    return "\n".join(lines)


# ============================================================================
# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³
# ============================================================================

_llm_client: Optional[LLMClient] = None

def get_llm_client() -> LLMClient:
    """LLMClientã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚’å–å¾—"""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client
