"""
Policy/Value Learning: Metamonå¼ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å­¦ç¿’

ãƒ­ã‚°ã‹ã‚‰ Policyï¼ˆè¡Œå‹•åˆ†å¸ƒï¼‰ã¨ Valueï¼ˆå‹ç‡ï¼‰ã‚’å­¦ç¿’ã™ã‚‹ã€‚

ç ”ç©¶å‚ç…§:
- Metamon: https://arxiv.org/abs/2504.04395
  - è¦³æˆ¦ãƒ­ã‚°ã‹ã‚‰å­¦ç¿’å¯èƒ½ãªè»Œè·¡ã‚’ä½œã‚Š
  - ã¾ãšæ¨¡å€£â†’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³RLâ†’è‡ªå·±å¯¾æˆ¦ã§å¾®èª¿æ•´

å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º:
1. ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆBattleLog â†’ TrainingExampleï¼‰
2. Policyå­¦ç¿’ï¼ˆè¡Œå‹•åˆ†é¡ï¼‰
3. Valueå­¦ç¿’ï¼ˆå‹ç‡å›å¸°ï¼‰
"""

from __future__ import annotations

import json
import os
import pickle
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np

try:
    import lightgbm as lgb
    HAS_LIGHTGBM = True
except ImportError:
    HAS_LIGHTGBM = False


# ============================================================================
# ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
# ============================================================================

@dataclass
class StateFeatures:
    """
    çŠ¶æ…‹ã®ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«
    
    ã‚ªãƒ¼ãƒ—ãƒ³ãƒãƒ¼ãƒ ã‚·ãƒ¼ãƒˆå‰æãªã®ã§ã€å…¨æƒ…å ±ã‚’ç‰¹å¾´é‡åŒ–ã§ãã‚‹ã€‚
    """
    # è‡ªåˆ†ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ï¼ˆ2ä½“ï¼‰
    self_hp: List[float]  # [slot0_hp, slot1_hp]
    self_status: List[int]  # [slot0_status_code, slot1_status_code]
    self_boosts: List[Dict[str, int]]  # ãƒ©ãƒ³ã‚¯å¤‰åŒ–
    
    # ç›¸æ‰‹ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ï¼ˆ2ä½“ï¼‰
    opp_hp: List[float]
    opp_status: List[int]
    opp_boosts: List[Dict[str, int]]
    
    # æ§ãˆæƒ…å ±
    self_reserves: int  # æ®‹ã‚Šæ§ãˆæ•°
    opp_reserves: int
    
    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰çŠ¶æ…‹
    weather: int  # å¤©å€™ã‚³ãƒ¼ãƒ‰
    terrain: int  # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚³ãƒ¼ãƒ‰
    trick_room: int  # 0 or æ®‹ã‚Šã‚¿ãƒ¼ãƒ³
    tailwind_self: int  # 0 or æ®‹ã‚Šã‚¿ãƒ¼ãƒ³
    tailwind_opp: int
    
    # ã‚¿ãƒ¼ãƒ³æƒ…å ±
    turn: int
    
    def to_vector(self) -> np.ndarray:
        """ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
        features = []
        
        # HP (4)
        features.extend(self.self_hp)
        features.extend(self.opp_hp)
        
        # Status (4)
        features.extend(self.self_status)
        features.extend(self.opp_status)
        
        # Reserves (2)
        features.append(self.self_reserves)
        features.append(self.opp_reserves)
        
        # Field (5)
        features.append(self.weather)
        features.append(self.terrain)
        features.append(self.trick_room)
        features.append(self.tailwind_self)
        features.append(self.tailwind_opp)
        
        # Turn (1)
        features.append(self.turn)
        
        # Boosts (å„6é …ç›® Ã— 4ä½“ = 24)
        boost_keys = ['atk', 'def', 'spa', 'spd', 'spe', 'accuracy']
        for boosts in self.self_boosts + self.opp_boosts:
            for key in boost_keys:
                features.append(boosts.get(key, 0))
        
        return np.array(features, dtype=np.float32)
    
    @staticmethod
    def feature_dim() -> int:
        """ç‰¹å¾´é‡æ¬¡å…ƒæ•°"""
        return 4 + 4 + 2 + 5 + 1 + 24  # = 40


@dataclass
class ActionLabel:
    """
    è¡Œå‹•ãƒ©ãƒ™ãƒ«ï¼ˆPolicyå­¦ç¿’ç”¨ï¼‰
    
    2ä½“åˆ†ã®è¡Œå‹•ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    """
    slot0_action_id: int  # è¡Œå‹•ID
    slot1_action_id: int
    
    @staticmethod
    def from_joint_action(action: Any, action_vocab: Dict[str, int]) -> 'ActionLabel':
        """JointActionã‹ã‚‰ãƒ©ãƒ™ãƒ«ã‚’ä½œæˆ"""
        slot0_key = f"{action.slot0_action.action_type}:{action.slot0_action.move_or_pokemon}"
        slot1_key = f"{action.slot1_action.action_type}:{action.slot1_action.move_or_pokemon}"
        
        return ActionLabel(
            slot0_action_id=action_vocab.get(slot0_key, 0),
            slot1_action_id=action_vocab.get(slot1_key, 0),
        )


@dataclass
class TrainingExample:
    """å­¦ç¿’ç”¨ã‚µãƒ³ãƒ—ãƒ«"""
    state: StateFeatures
    action: ActionLabel  # Policyå­¦ç¿’ç”¨
    outcome: float  # Valueå­¦ç¿’ç”¨ï¼ˆ1.0=å‹åˆ©, 0.0=æ•—åŒ—, 0.5=å¼•ãåˆ†ã‘ï¼‰
    side: str  # "p1" or "p2"


@dataclass
class BattleLog:
    """ãƒãƒˆãƒ«ãƒ­ã‚°ï¼ˆ1è©¦åˆåˆ†ï¼‰"""
    battle_id: str
    format: str
    winner: str  # "p1" or "p2"
    turns: List[TurnLog]
    
    def to_training_examples(self, action_vocab: Dict[str, int]) -> List[TrainingExample]:
        """å­¦ç¿’ç”¨ã‚µãƒ³ãƒ—ãƒ«ã«å¤‰æ›"""
        examples = []
        
        for turn in self.turns:
            for side in ["p1", "p2"]:
                state = turn.get_state_features(side)
                action = turn.get_action_label(side, action_vocab)
                outcome = 1.0 if self.winner == side else 0.0
                
                examples.append(TrainingExample(
                    state=state,
                    action=action,
                    outcome=outcome,
                    side=side,
                ))
        
        return examples


@dataclass
class TurnLog:
    """ã‚¿ãƒ¼ãƒ³ãƒ­ã‚°"""
    turn: int
    p1_state: Dict[str, Any]
    p2_state: Dict[str, Any]
    p1_action: Optional[Dict[str, Any]]
    p2_action: Optional[Dict[str, Any]]
    
    def get_state_features(self, side: str) -> StateFeatures:
        """çŠ¶æ…‹ç‰¹å¾´é‡ã‚’å–å¾—"""
        if side == "p1":
            self_state = self.p1_state
            opp_state = self.p2_state
        else:
            self_state = self.p2_state
            opp_state = self.p1_state
        
        return StateFeatures(
            self_hp=self_state.get("hp", [1.0, 1.0]),
            self_status=self_state.get("status", [0, 0]),
            self_boosts=self_state.get("boosts", [{}, {}]),
            opp_hp=opp_state.get("hp", [1.0, 1.0]),
            opp_status=opp_state.get("status", [0, 0]),
            opp_boosts=opp_state.get("boosts", [{}, {}]),
            self_reserves=self_state.get("reserves", 2),
            opp_reserves=opp_state.get("reserves", 2),
            weather=self_state.get("weather", 0),
            terrain=self_state.get("terrain", 0),
            trick_room=self_state.get("trick_room", 0),
            tailwind_self=self_state.get("tailwind", 0),
            tailwind_opp=opp_state.get("tailwind", 0),
            turn=self.turn,
        )
    
    def get_action_label(self, side: str, action_vocab: Dict[str, int]) -> ActionLabel:
        """è¡Œå‹•ãƒ©ãƒ™ãƒ«ã‚’å–å¾—"""
        if side == "p1":
            action = self.p1_action
        else:
            action = self.p2_action
        
        if not action:
            return ActionLabel(slot0_action_id=0, slot1_action_id=0)
        
        slot0_key = f"{action.get('slot0_type', 'move')}:{action.get('slot0_move', 'tackle')}"
        slot1_key = f"{action.get('slot1_type', 'move')}:{action.get('slot1_move', 'tackle')}"
        
        return ActionLabel(
            slot0_action_id=action_vocab.get(slot0_key, 0),
            slot1_action_id=action_vocab.get(slot1_key, 0),
        )


# ============================================================================
# Policy Modelï¼ˆè¡Œå‹•äºˆæ¸¬ï¼‰
# ============================================================================

class PolicyModel:
    """
    Policyå­¦ç¿’ãƒ¢ãƒ‡ãƒ«
    
    çŠ¶æ…‹ã‹ã‚‰è¡Œå‹•åˆ†å¸ƒã‚’äºˆæ¸¬ï¼ˆæ¨¡å€£å­¦ç¿’ï¼‰
    """
    
    def __init__(self, action_vocab_size: int = 500):
        self.action_vocab_size = action_vocab_size
        self.model_slot0 = None  # LightGBM for slot0
        self.model_slot1 = None  # LightGBM for slot1
        self.action_vocab: Dict[str, int] = {}
        self.id_to_action: Dict[int, str] = {}
    
    def train(self, examples: List[TrainingExample], **lgb_params):
        """å­¦ç¿’"""
        if not HAS_LIGHTGBM:
            print("âš ï¸ LightGBM not installed, skipping training")
            return
        
        X = np.array([ex.state.to_vector() for ex in examples])
        y_slot0 = np.array([ex.action.slot0_action_id for ex in examples])
        y_slot1 = np.array([ex.action.slot1_action_id for ex in examples])
        
        default_params = {
            "objective": "multiclass",
            "num_class": self.action_vocab_size,
            "metric": "multi_logloss",
            "verbosity": -1,
            "num_leaves": 31,
            "learning_rate": 0.05,
            "n_estimators": 100,
        }
        default_params.update(lgb_params)
        
        print(f"Training Policy (slot0) on {len(examples)} examples...")
        self.model_slot0 = lgb.LGBMClassifier(**default_params)
        self.model_slot0.fit(X, y_slot0)
        
        print(f"Training Policy (slot1) on {len(examples)} examples...")
        self.model_slot1 = lgb.LGBMClassifier(**default_params)
        self.model_slot1.fit(X, y_slot1)
        
        print("âœ… Policy training complete")
    
    def predict_proba(self, state: StateFeatures) -> Tuple[np.ndarray, np.ndarray]:
        """è¡Œå‹•ç¢ºç‡ã‚’äºˆæ¸¬"""
        if self.model_slot0 is None or self.model_slot1 is None:
            # æœªå­¦ç¿’ã®å ´åˆã¯ä¸€æ§˜åˆ†å¸ƒ
            uniform = np.ones(self.action_vocab_size) / self.action_vocab_size
            return uniform, uniform
        
        X = state.to_vector().reshape(1, -1)
        proba_slot0 = self.model_slot0.predict_proba(X)[0]
        proba_slot1 = self.model_slot1.predict_proba(X)[0]
        
        return proba_slot0, proba_slot1
    
    def save(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        with open(path, 'wb') as f:
            pickle.dump({
                'model_slot0': self.model_slot0,
                'model_slot1': self.model_slot1,
                'action_vocab': self.action_vocab,
                'id_to_action': self.id_to_action,
            }, f)
        print(f"âœ… Policy model saved to {path}")
    
    def load(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        with open(path, 'rb') as f:
            data = pickle.load(f)
            self.model_slot0 = data['model_slot0']
            self.model_slot1 = data['model_slot1']
            self.action_vocab = data['action_vocab']
            self.id_to_action = data['id_to_action']
        print(f"âœ… Policy model loaded from {path}")


# ============================================================================
# Value Modelï¼ˆå‹ç‡äºˆæ¸¬ï¼‰
# ============================================================================

class ValueModel:
    """
    Valueå­¦ç¿’ãƒ¢ãƒ‡ãƒ«
    
    çŠ¶æ…‹ã‹ã‚‰å‹ç‡ã‚’äºˆæ¸¬
    """
    
    def __init__(self):
        self.model = None
    
    def train(self, examples: List[TrainingExample], **lgb_params):
        """å­¦ç¿’"""
        if not HAS_LIGHTGBM:
            print("âš ï¸ LightGBM not installed, skipping training")
            return
        
        X = np.array([ex.state.to_vector() for ex in examples])
        y = np.array([ex.outcome for ex in examples])
        
        default_params = {
            "objective": "binary",
            "metric": "auc",
            "verbosity": -1,
            "num_leaves": 31,
            "learning_rate": 0.05,
            "n_estimators": 100,
        }
        default_params.update(lgb_params)
        
        print(f"Training Value model on {len(examples)} examples...")
        self.model = lgb.LGBMClassifier(**default_params)
        self.model.fit(X, y)
        
        print("âœ… Value training complete")
    
    def predict(self, state: StateFeatures) -> float:
        """å‹ç‡ã‚’äºˆæ¸¬"""
        if self.model is None:
            return 0.5  # æœªå­¦ç¿’ã®å ´åˆã¯0.5
        
        X = state.to_vector().reshape(1, -1)
        proba = self.model.predict_proba(X)[0]
        return proba[1] if len(proba) > 1 else 0.5
    
    def save(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        with open(path, 'wb') as f:
            pickle.dump({'model': self.model}, f)
        print(f"âœ… Value model saved to {path}")
    
    def load(self, path: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        with open(path, 'rb') as f:
            data = pickle.load(f)
            self.model = data['model']
        print(f"âœ… Value model loaded from {path}")


# ============================================================================
# ãƒ­ã‚°åé›†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================================

class BattleLogCollector:
    """
    ãƒãƒˆãƒ«ãƒ­ã‚°ã‚’åé›†ãƒ»å¤‰æ›ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    
    Showdownã®ãƒ­ã‚°ã‹ã‚‰å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    """
    
    def __init__(self, log_dir: str = "data/battle_logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.action_vocab: Dict[str, int] = {"unknown": 0}
        self._next_action_id = 1
    
    def register_action(self, action_key: str) -> int:
        """è¡Œå‹•ã‚’èªå½™ã«ç™»éŒ²"""
        if action_key not in self.action_vocab:
            self.action_vocab[action_key] = self._next_action_id
            self._next_action_id += 1
        return self.action_vocab[action_key]
    
    def parse_showdown_log(self, log_text: str) -> Optional[BattleLog]:
        """Showdownã®ãƒ­ã‚°ã‚’ãƒ‘ãƒ¼ã‚¹"""
        # TODO: å®Ÿéš›ã®ãƒ‘ãƒ¼ã‚¹å®Ÿè£…
        # ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: |turn|1, |move|p1a: Raichu|Thunderbolt|p2a: Gyarados, etc.
        return None
    
    def collect_from_file(self, filepath: str) -> List[TrainingExample]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ­ã‚°ã‚’åé›†"""
        try:
            with open(filepath, 'r') as f:
                log_text = f.read()
            
            battle_log = self.parse_showdown_log(log_text)
            if battle_log:
                return battle_log.to_training_examples(self.action_vocab)
        except Exception as e:
            print(f"âš ï¸ Failed to parse {filepath}: {e}")
        
        return []
    
    def collect_all(self) -> List[TrainingExample]:
        """å…¨ãƒ­ã‚°ã‚’åé›†"""
        examples = []
        for log_file in self.log_dir.glob("*.log"):
            examples.extend(self.collect_from_file(str(log_file)))
        return examples
    
    def save_vocab(self, path: str):
        """èªå½™ã‚’ä¿å­˜"""
        with open(path, 'w') as f:
            json.dump(self.action_vocab, f, indent=2)


# ============================================================================
# çµ±åˆãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
# ============================================================================

class MetamonTrainer:
    """
    Metamonå¼ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
    
    ä½¿ç”¨æ–¹æ³•:
    1. trainer = MetamonTrainer()
    2. trainer.collect_logs("path/to/logs")
    3. trainer.train()
    4. trainer.save_models()
    """
    
    def __init__(self, model_dir: str = "models"):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        self.collector = BattleLogCollector()
        self.policy_model = PolicyModel()
        self.value_model = ValueModel()
        self.examples: List[TrainingExample] = []
    
    def collect_logs(self, log_dir: str):
        """ãƒ­ã‚°ã‚’åé›†"""
        self.collector.log_dir = Path(log_dir)
        self.examples = self.collector.collect_all()
        print(f"ğŸ“Š Collected {len(self.examples)} training examples")
    
    def train(self, policy_params: Optional[Dict] = None, value_params: Optional[Dict] = None):
        """å­¦ç¿’å®Ÿè¡Œ"""
        if not self.examples:
            print("âš ï¸ No training examples. Run collect_logs() first.")
            return
        
        self.policy_model.action_vocab = self.collector.action_vocab
        self.policy_model.id_to_action = {v: k for k, v in self.collector.action_vocab.items()}
        
        self.policy_model.train(self.examples, **(policy_params or {}))
        self.value_model.train(self.examples, **(value_params or {}))
    
    def save_models(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        self.policy_model.save(str(self.model_dir / "policy_model.pkl"))
        self.value_model.save(str(self.model_dir / "value_model.pkl"))
        self.collector.save_vocab(str(self.model_dir / "action_vocab.json"))
    
    def load_models(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        self.policy_model.load(str(self.model_dir / "policy_model.pkl"))
        self.value_model.load(str(self.model_dir / "value_model.pkl"))


# ============================================================================
# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³
# ============================================================================

_policy_model: Optional[PolicyModel] = None
_value_model: Optional[ValueModel] = None

def get_policy_model() -> PolicyModel:
    """PolicyModel ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚’å–å¾—"""
    global _policy_model
    if _policy_model is None:
        _policy_model = PolicyModel()
        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿
        model_path = Path("models/policy_model.pkl")
        if model_path.exists():
            _policy_model.load(str(model_path))
    return _policy_model

def get_value_model() -> ValueModel:
    """ValueModel ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚’å–å¾—"""
    global _value_model
    if _value_model is None:
        _value_model = ValueModel()
        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿
        model_path = Path("models/value_model.pkl")
        if model_path.exists():
            _value_model.load(str(model_path))
    return _value_model
