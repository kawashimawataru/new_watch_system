#!/usr/bin/env python3
"""
VGCå…¨ãƒ¬ã‚®ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒªãƒ—ãƒ¬ã‚¤ã‚’åŠ¹ç‡çš„ã«åé›†

ä½¿ç”¨æ–¹æ³•:
    python scripts/download_vgc_replays.py --count 1000 --output data/replays
"""

import argparse
import asyncio
import json
import logging
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

import aiohttp


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class VGCReplayDownloader:
    """VGCå…¨ãƒ¬ã‚®ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒªãƒ—ãƒ¬ã‚¤ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼"""
    
    BASE_URL = "https://replay.pokemonshowdown.com"
    
    # VGC ãƒ¬ã‚®ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ–°ã—ã„é †ï¼‰
    # æ³¨: 2026 Reg Fã¯ã¾ã æœªå®Ÿè£…ã®å¯èƒ½æ€§ã‚ã‚Š
    VGC_FORMATS = [
        "gen9vgc2025regj",      # æœ€æ–°ï¼ˆ2025å¹´10æœˆï½ï¼‰
        "gen9vgc2025regi",      # 2025å¹´7æœˆï½9æœˆ
        "gen9vgc2025regh",      # 2025å¹´å‰åŠ
        "gen9vgc2024regg",      # 2024å¹´
        "gen9vgc2023regd",      # 2023å¹´å¾ŒåŠ
        "gen9vgc2023regc",      # 2023å¹´å‰åŠ
    ]
    
    def __init__(self, min_rating: int = 1500):
        """
        Args:
            min_rating: æœ€ä½ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
        """
        self.min_rating = min_rating
        self.downloaded_count = 0
        self.failed_count = 0
        self.format_stats = {fmt: 0 for fmt in self.VGC_FORMATS}
    
    async def search_replays_for_format(
        self,
        session: aiohttp.ClientSession,
        format_id: str,
        page: int = 1
    ) -> List[Dict[str, Any]]:
        """
        æŒ‡å®šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒªãƒ—ãƒ¬ã‚¤ã‚’æ¤œç´¢
        
        Args:
            session: aiohttp ã‚»ãƒƒã‚·ãƒ§ãƒ³
            format_id: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆID
            page: ãƒšãƒ¼ã‚¸ç•ªå·
            
        Returns:
            ãƒªãƒ—ãƒ¬ã‚¤æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        url = f"{self.BASE_URL}/search.json"
        params = {
            "format": format_id,
            "page": page,
        }
        
        try:
            async with session.get(url, params=params, timeout=30) as response:
                if response.status != 200:
                    logger.warning(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({format_id}, page {page}): HTTP {response.status}")
                    return []
                
                data = await response.json()
                
                # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                if page == 1 and isinstance(data, list) and len(data) > 0:
                    logger.info(f"æ¤œç´¢æˆåŠŸ ({format_id}): {len(data)}ä»¶")
                
                replays = data if isinstance(data, list) else []
                
                return replays
                
        except Exception as e:
            logger.warning(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({format_id}, page {page}): {e}")
            return []
    
    async def download_replay_detail(
        self,
        session: aiohttp.ClientSession,
        replay_id: str
    ) -> Dict[str, Any] | None:
        """
        ãƒªãƒ—ãƒ¬ã‚¤ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        
        Args:
            session: aiohttp ã‚»ãƒƒã‚·ãƒ§ãƒ³
            replay_id: ãƒªãƒ—ãƒ¬ã‚¤ID
            
        Returns:
            ãƒªãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿
        """
        url = f"{self.BASE_URL}/{replay_id}.json"
        
        try:
            async with session.get(url, timeout=30) as response:
                if response.status != 200:
                    self.failed_count += 1
                    return None
                
                data = await response.json()
                
                # ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒã‚§ãƒƒã‚¯
                rating = data.get("rating")
                if rating is None or rating < self.min_rating:
                    return None
                
                # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆçµ±è¨ˆ
                format_id = data.get("format", "")
                if format_id in self.format_stats:
                    self.format_stats[format_id] += 1
                
                self.downloaded_count += 1
                
                if self.downloaded_count % 10 == 0:
                    logger.info(
                        f"é€²æ—: {self.downloaded_count}ä»¶ "
                        f"(æœ€æ–°: {replay_id}, Rating: {rating})"
                    )
                
                return {
                    "id": replay_id,
                    "format": format_id,
                    "rating": rating,
                    "uploadtime": data.get("uploadtime"),
                    "log": data.get("log", ""),
                    "players": data.get("players", []),
                    "winner": data.get("winner"),
                }
                
        except Exception as e:
            self.failed_count += 1
            logger.debug(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•— ({replay_id}): {e}")
            return None
    
    async def collect_replays_parallel(
        self,
        target_count: int,
        max_concurrent: int = 10
    ) -> List[Dict[str, Any]]:
        """
        è¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰ä¸¦åˆ—ã§ãƒªãƒ—ãƒ¬ã‚¤ã‚’åé›†
        
        Args:
            target_count: åé›†ç›®æ¨™æ•°
            max_concurrent: æœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°
            
        Returns:
            ãƒªãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        replays_data = []
        seen_ids = set()
        
        async with aiohttp.ClientSession() as session:
            # å„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æ¤œç´¢ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œ
            format_pages = {fmt: 1 for fmt in self.VGC_FORMATS}
            
            while self.downloaded_count < target_count:
                # å„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰æ¤œç´¢
                search_tasks = []
                for format_id in self.VGC_FORMATS:
                    if format_pages[format_id] <= 10:  # å„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæœ€å¤§10ãƒšãƒ¼ã‚¸
                        search_tasks.append(
                            self.search_replays_for_format(
                                session,
                                format_id,
                                format_pages[format_id]
                            )
                        )
                        format_pages[format_id] += 1
                
                if not search_tasks:
                    logger.warning("âš ï¸  æ¤œç´¢å¯èƒ½ãªãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“")
                    break
                
                # ä¸¦åˆ—æ¤œç´¢å®Ÿè¡Œ
                search_results = await asyncio.gather(*search_tasks)
                
                # ãƒªãƒ—ãƒ¬ã‚¤IDã‚’åé›†
                replay_ids = []
                for replays in search_results:
                    for item in replays:
                        replay_id = item.get("id", "")
                        if replay_id and replay_id not in seen_ids:
                            seen_ids.add(replay_id)
                            replay_ids.append(replay_id)
                
                if not replay_ids:
                    logger.warning("âš ï¸  æ–°ã—ã„ãƒªãƒ—ãƒ¬ã‚¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    break
                
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œ
                download_tasks = []
                for replay_id in replay_ids[:max_concurrent]:
                    if self.downloaded_count >= target_count:
                        break
                    download_tasks.append(
                        self.download_replay_detail(session, replay_id)
                    )
                
                download_results = await asyncio.gather(*download_tasks)
                
                # æˆåŠŸã—ãŸã‚‚ã®ã‚’ä¿å­˜
                for replay_data in download_results:
                    if replay_data:
                        replays_data.append(replay_data)
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                await asyncio.sleep(0.5)
        
        return replays_data
    
    def save_batch(
        self,
        replays_data: List[Dict[str, Any]],
        output_dir: Path
    ):
        """
        ãƒªãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒä¿å­˜
        
        Args:
            replays_data: ãƒªãƒ—ãƒ¬ã‚¤ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            output_dir: ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"vgc_replays_{timestamp}.json"
        filepath = output_dir / filename
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(replays_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ä¿å­˜: {filepath} ({len(replays_data)}ä»¶)")
    
    def print_statistics(self):
        """çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š åé›†çµ±è¨ˆ")
        logger.info("="*60)
        logger.info(f"æˆåŠŸ: {self.downloaded_count}ä»¶")
        logger.info(f"å¤±æ•—: {self.failed_count}ä»¶")
        logger.info("\nãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ¥å†…è¨³:")
        for format_id in self.VGC_FORMATS:
            count = self.format_stats[format_id]
            if count > 0:
                logger.info(f"  {format_id}: {count}ä»¶")


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description="VGCå…¨ãƒ¬ã‚®ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒªãƒ—ãƒ¬ã‚¤ã‚’åé›†"
    )
    parser.add_argument(
        "--count",
        type=int,
        default=1000,
        help="åé›†ã™ã‚‹ãƒªãƒ—ãƒ¬ã‚¤æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000)"
    )
    parser.add_argument(
        "--min-rating",
        type=int,
        default=1500,
        help="æœ€ä½ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1500)"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("data/replays"),
        help="ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: data/replays)"
    )
    parser.add_argument(
        "--concurrent",
        type=int,
        default=10,
        help="æœ€å¤§åŒæ™‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10)"
    )
    
    args = parser.parse_args()
    
    logger.info("="*70)
    logger.info("ğŸš€ VGC ãƒªãƒ—ãƒ¬ã‚¤åé›†é–‹å§‹")
    logger.info("="*70)
    logger.info(f"å¯¾è±¡ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: å…¨VGCãƒ¬ã‚®ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    logger.info(f"ç›®æ¨™ä»¶æ•°: {args.count}ä»¶")
    logger.info(f"æœ€ä½ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°: {args.min_rating}")
    logger.info(f"ä¿å­˜å…ˆ: {args.output}")
    logger.info(f"åŒæ™‚å®Ÿè¡Œæ•°: {args.concurrent}")
    logger.info("")
    
    downloader = VGCReplayDownloader(min_rating=args.min_rating)
    
    try:
        # ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
        replays = await downloader.collect_replays_parallel(
            target_count=args.count,
            max_concurrent=args.concurrent
        )
        
        # ä¿å­˜
        if replays:
            downloader.save_batch(replays, args.output)
        
        # çµ±è¨ˆè¡¨ç¤º
        downloader.print_statistics()
        
        logger.info("\nâœ… Phase 2 å®Œäº†ï¼")
        
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        logger.info(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {downloader.downloaded_count}ä»¶")
        
        # ä¸­æ–­æ™‚ã‚‚ä¿å­˜
        if downloader.downloaded_count > 0:
            logger.info("åé›†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­...")
            # æ³¨: å®Ÿè£…ä¸Šã€ã“ã“ã§ã¯ä¿å­˜æ¸ˆã¿
    
    except Exception as e:
        logger.error(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
