#!/usr/bin/env python3
"""
é«˜ãƒ¬ãƒ¼ãƒˆå¸¯VGCãƒªãƒ—ãƒ¬ã‚¤åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (1500+ç‰¹åŒ–)

Pokemon Showdownã‹ã‚‰é«˜ãƒ¬ãƒ¼ãƒˆå¸¯(1500+)ã®VGCãƒªãƒ—ãƒ¬ã‚¤ã‚’åŠ¹ç‡çš„ã«åé›†ã—ã¾ã™ã€‚
é€šå¸¸ã®æ¤œç´¢APIã§ã¯ãƒ¬ãƒ¼ãƒˆæŒ‡å®šãŒã§ããªã„ãŸã‚ã€å¤§é‡ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/download_high_rating_replays.py --target 200 --min-rating 1500
"""

import argparse
import asyncio
import json
import logging
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

import aiohttp


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class HighRatingReplayCollector:
    """é«˜ãƒ¬ãƒ¼ãƒˆå¸¯ãƒªãƒ—ãƒ¬ã‚¤å°‚ç”¨ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼"""
    
    BASE_URL = "https://replay.pokemonshowdown.com"
    
    # å…¨VGCãƒ¬ã‚®ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ (ãƒ¬ãƒ¼ãƒˆå•ã‚ãšåé›†)
    VGC_FORMATS = [
        "gen9vgc2025regj",      # æœ€æ–° (2025å¹´10æœˆï½)
        "gen9vgc2025regi",      # 2025å¹´7ï½9æœˆ
        "gen9vgc2025regh",      # 2025å¹´å‰åŠ
        "gen9vgc2024regg",      # 2024å¹´
        "gen9vgc2023regd",      # 2023å¹´å¾ŒåŠ
        "gen9vgc2023regc",      # 2023å¹´å‰åŠ
    ]
    
    def __init__(self, min_rating: int = 1500, target_count: int = 200):
        """
        Args:
            min_rating: æœ€ä½ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
            target_count: åé›†ç›®æ¨™æ•°
        """
        self.min_rating = min_rating
        self.target_count = target_count
        self.collected_replays = []
        self.downloaded_count = 0
        self.filtered_count = 0
        self.rating_distribution = {
            "1500-1600": 0,
            "1600-1700": 0,
            "1700-1800": 0,
            "1800+": 0,
        }
    
    async def search_replays(
        self,
        session: aiohttp.ClientSession,
        format_id: str,
        page: int = 1
    ) -> List[Dict[str, Any]]:
        """ãƒªãƒ—ãƒ¬ã‚¤æ¤œç´¢"""
        url = f"{self.BASE_URL}/search.json"
        params = {"format": format_id, "page": page}
        
        try:
            async with session.get(url, params=params, timeout=30) as response:
                if response.status != 200:
                    return []
                
                data = await response.json()
                return data if isinstance(data, list) else []
                
        except Exception as e:
            logger.debug(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({format_id}, page {page}): {e}")
            return []
    
    async def download_replay_detail(
        self,
        session: aiohttp.ClientSession,
        replay_id: str
    ) -> Dict[str, Any] | None:
        """ãƒªãƒ—ãƒ¬ã‚¤è©³ç´°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        url = f"{self.BASE_URL}/{replay_id}.json"
        
        try:
            async with session.get(url, timeout=30) as response:
                if response.status != 200:
                    return None
                
                data = await response.json()
                rating = data.get("rating")
                
                self.downloaded_count += 1
                
                # ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ•ã‚£ãƒ«ã‚¿
                if rating is None or rating < self.min_rating:
                    self.filtered_count += 1
                    return None
                
                # ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°åˆ†å¸ƒã‚’è¨˜éŒ²
                if rating >= 1800:
                    self.rating_distribution["1800+"] += 1
                elif rating >= 1700:
                    self.rating_distribution["1700-1800"] += 1
                elif rating >= 1600:
                    self.rating_distribution["1600-1700"] += 1
                else:
                    self.rating_distribution["1500-1600"] += 1
                
                if len(self.collected_replays) % 10 == 0 and len(self.collected_replays) > 0:
                    logger.info(
                        f"âœ… é€²æ—: {len(self.collected_replays)}/{self.target_count} "
                        f"(Rating: {rating}, DL: {self.downloaded_count}, "
                        f"é™¤å¤–: {self.filtered_count})"
                    )
                
                return {
                    "id": replay_id,
                    "format": data.get("format"),
                    "rating": rating,
                    "uploadtime": data.get("uploadtime"),
                    "log": data.get("log", ""),
                    "players": data.get("players", []),
                    "winner": data.get("winner"),
                }
                
        except Exception as e:
            logger.debug(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•— ({replay_id}): {e}")
            return None
    
    async def collect_high_rating_replays(
        self,
        max_concurrent: int = 20,
        max_pages_per_format: int = 30
    ) -> List[Dict[str, Any]]:
        """
        é«˜ãƒ¬ãƒ¼ãƒˆå¸¯ãƒªãƒ—ãƒ¬ã‚¤ã‚’åé›†
        
        æˆ¦ç•¥:
        1. è¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰ä¸¦åˆ—æ¤œç´¢
        2. å¤§é‡ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ¬ãƒ¼ãƒˆã§ãƒ•ã‚£ãƒ«ã‚¿
        3. ç›®æ¨™æ•°ã«é”ã—ãŸã‚‰çµ‚äº†
        """
        logger.info(f"ğŸ¯ ç›®æ¨™: ãƒ¬ãƒ¼ãƒˆ{self.min_rating}+ã®ãƒªãƒ—ãƒ¬ã‚¤ã‚’{self.target_count}ä»¶åé›†")
        
        async with aiohttp.ClientSession() as session:
            format_pages = {fmt: 1 for fmt in self.VGC_FORMATS}
            
            while len(self.collected_replays) < self.target_count:
                # å„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰æ¤œç´¢
                search_tasks = []
                for format_id in self.VGC_FORMATS:
                    if format_pages[format_id] <= max_pages_per_format:
                        search_tasks.append(
                            self.search_replays(session, format_id, format_pages[format_id])
                        )
                        format_pages[format_id] += 1
                
                if not search_tasks:
                    logger.warning("âš ï¸  æ¤œç´¢å¯èƒ½ãªãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“")
                    break
                
                # ä¸¦åˆ—æ¤œç´¢
                search_results = await asyncio.gather(*search_tasks)
                
                # ãƒªãƒ—ãƒ¬ã‚¤IDã‚’åé›†
                replay_ids = []
                for replays in search_results:
                    for item in replays:
                        replay_id = item.get("id", "")
                        if replay_id:
                            replay_ids.append(replay_id)
                
                if not replay_ids:
                    break
                
                logger.info(f"ğŸ“¥ {len(replay_ids)}ä»¶ã®ãƒªãƒ—ãƒ¬ã‚¤ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")
                
                # ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (ãƒ¬ãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°)
                download_tasks = []
                for replay_id in replay_ids:
                    if len(self.collected_replays) >= self.target_count:
                        break
                    download_tasks.append(
                        self.download_replay_detail(session, replay_id)
                    )
                
                # ãƒãƒƒãƒå®Ÿè¡Œ
                for i in range(0, len(download_tasks), max_concurrent):
                    batch = download_tasks[i:i+max_concurrent]
                    results = await asyncio.gather(*batch)
                    
                    for replay_data in results:
                        if replay_data:
                            self.collected_replays.append(replay_data)
                            
                            if len(self.collected_replays) >= self.target_count:
                                break
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                    await asyncio.sleep(0.5)
                
                if len(self.collected_replays) >= self.target_count:
                    break
        
        return self.collected_replays
    
    def save_replays(self, output_dir: Path):
        """ãƒªãƒ—ãƒ¬ã‚¤ã‚’ä¿å­˜"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"vgc_high_rating_{self.min_rating}plus_{timestamp}.json"
        filepath = output_dir / filename
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(self.collected_replays, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ä¿å­˜: {filepath} ({len(self.collected_replays)}ä»¶)")
    
    def print_statistics(self):
        """çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š åé›†çµ±è¨ˆ")
        logger.info("="*60)
        logger.info(f"ç›®æ¨™: {self.target_count}ä»¶ (ãƒ¬ãƒ¼ãƒˆ{self.min_rating}+)")
        logger.info(f"åé›†æˆåŠŸ: {len(self.collected_replays)}ä»¶")
        logger.info(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç·æ•°: {self.downloaded_count}ä»¶")
        logger.info(f"ãƒ¬ãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ã§é™¤å¤–: {self.filtered_count}ä»¶")
        logger.info(f"æ¡ç”¨ç‡: {len(self.collected_replays)/max(self.downloaded_count,1)*100:.1f}%")
        logger.info("\nãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°åˆ†å¸ƒ:")
        for range_name, count in self.rating_distribution.items():
            if count > 0:
                logger.info(f"  {range_name}: {count}ä»¶")


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description="é«˜ãƒ¬ãƒ¼ãƒˆå¸¯VGCãƒªãƒ—ãƒ¬ã‚¤åé›† (1500+ç‰¹åŒ–)"
    )
    parser.add_argument(
        "--target",
        type=int,
        default=200,
        help="åé›†ç›®æ¨™æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 200)"
    )
    parser.add_argument(
        "--min-rating",
        type=int,
        default=1500,
        help="æœ€ä½ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1500)"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("data/replays"),
        help="ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: data/replays)"
    )
    parser.add_argument(
        "--concurrent",
        type=int,
        default=20,
        help="æœ€å¤§åŒæ™‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20)"
    )
    parser.add_argument(
        "--max-pages",
        type=int,
        default=30,
        help="å„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æœ€å¤§ãƒšãƒ¼ã‚¸æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 30)"
    )
    
    args = parser.parse_args()
    
    logger.info("="*70)
    logger.info("ğŸš€ é«˜ãƒ¬ãƒ¼ãƒˆå¸¯VGCãƒªãƒ—ãƒ¬ã‚¤åé›†")
    logger.info("="*70)
    logger.info(f"ç›®æ¨™ä»¶æ•°: {args.target}ä»¶")
    logger.info(f"æœ€ä½ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°: {args.min_rating}")
    logger.info(f"ä¿å­˜å…ˆ: {args.output}")
    logger.info("")
    
    collector = HighRatingReplayCollector(
        min_rating=args.min_rating,
        target_count=args.target
    )
    
    try:
        # åé›†å®Ÿè¡Œ
        replays = await collector.collect_high_rating_replays(
            max_concurrent=args.concurrent,
            max_pages_per_format=args.max_pages
        )
        
        # ä¿å­˜
        if replays:
            collector.save_replays(args.output)
        
        # çµ±è¨ˆè¡¨ç¤º
        collector.print_statistics()
        
        logger.info("\nâœ… åé›†å®Œäº†ï¼")
        
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        logger.info(f"åé›†æ¸ˆã¿: {len(collector.collected_replays)}ä»¶")
        
        # ä¸­æ–­æ™‚ã‚‚ä¿å­˜
        if collector.collected_replays:
            collector.save_replays(args.output)
    
    except Exception as e:
        logger.error(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
